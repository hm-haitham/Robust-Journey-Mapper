{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homies App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'jars': ['/user/chahed/graphframes-0.6.0-spark2.3-s_2.11.jar', '/user/chahed/scala-logging-api_2.11-2.1.2.jar', '/user/chahed/scala-logging-slf4j_2.11-2.1.2.jar'], 'pyFiles': ['/user/chahed/graphframes-0.6.0-spark2.3-s_2.11.jar'], 'conf': {'spark.app.name': 'homies_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8983</td><td>application_1589299642358_3520</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3520/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3520_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8996</td><td>application_1589299642358_3534</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3534/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3534_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9028</td><td>application_1589299642358_3573</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3573/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3573_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9054</td><td>application_1589299642358_3599</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3599/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3599_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9055</td><td>application_1589299642358_3601</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3601/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3601_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9065</td><td>application_1589299642358_3611</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3611/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3611_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9073</td><td>application_1589299642358_3622</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3622/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3622_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9074</td><td>application_1589299642358_3623</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3623/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3623_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9081</td><td>application_1589299642358_3641</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3641/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3641_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9084</td><td>application_1589299642358_3644</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3644/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3644_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9089</td><td>application_1589299642358_3650</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3650/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3650_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9091</td><td>application_1589299642358_3652</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3652/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3652_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9093</td><td>application_1589299642358_3654</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3654/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3654_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9094</td><td>application_1589299642358_3655</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3655/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3655_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9096</td><td>application_1589299642358_3657</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3657/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3657_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9098</td><td>application_1589299642358_3660</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3660/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3660_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9099</td><td>application_1589299642358_3661</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3661/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3661_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9100</td><td>application_1589299642358_3662</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3662/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3662_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9102</td><td>application_1589299642358_3664</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3664/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3664_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9104</td><td>application_1589299642358_3666</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3666/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3666_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9105</td><td>application_1589299642358_3667</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3667/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3667_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9106</td><td>application_1589299642358_3668</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3668/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3668_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9108</td><td>application_1589299642358_3671</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3671/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3671_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9109</td><td>application_1589299642358_3672</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3672/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3672_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9110</td><td>application_1589299642358_3673</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3673/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3673_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9111</td><td>application_1589299642358_3674</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3674/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3674_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9112</td><td>application_1589299642358_3675</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3675/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3675_01_000005/ebouille\">Link</a></td><td></td></tr><tr><td>9113</td><td>application_1589299642358_3676</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3676/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3676_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9114</td><td>application_1589299642358_3677</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3677/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3677_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9115</td><td>application_1589299642358_3678</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3678/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3678_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9117</td><td>application_1589299642358_3680</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3680/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3680_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9118</td><td>application_1589299642358_3681</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3681/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3681_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9119</td><td>application_1589299642358_3682</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3682/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3682_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9120</td><td>application_1589299642358_3683</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3683/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3683_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9121</td><td>application_1589299642358_3684</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3684/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3684_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9129</td><td>application_1589299642358_3692</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3692/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3692_01_000002/ebouille\">Link</a></td><td></td></tr><tr><td>9130</td><td>application_1589299642358_3694</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3694/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3694_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9131</td><td>application_1589299642358_3695</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3695/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3695_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9132</td><td>application_1589299642358_3696</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3696/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3696_01_000002/ebouille\">Link</a></td><td></td></tr><tr><td>9133</td><td>application_1589299642358_3697</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3697/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3697_01_000002/ebouille\">Link</a></td><td></td></tr><tr><td>9134</td><td>application_1589299642358_3698</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3698/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3698_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9135</td><td>application_1589299642358_3699</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3699/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3699_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9138</td><td>application_1589299642358_3702</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3702/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3702_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9139</td><td>application_1589299642358_3703</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3703/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3703_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9140</td><td>application_1589299642358_3704</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3704/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3704_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9141</td><td>application_1589299642358_3705</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3705/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3705_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"jars\": [\"/user/chahed/graphframes-0.6.0-spark2.3-s_2.11.jar\", \n",
    "             \"/user/chahed/scala-logging-api_2.11-2.1.2.jar\",\n",
    "             \"/user/chahed/scala-logging-slf4j_2.11-2.1.2.jar\"],\n",
    "    \"pyFiles\": [\"/user/chahed/graphframes-0.6.0-spark2.3-s_2.11.jar\"],\n",
    "    \"conf\": {\n",
    "        \"spark.app.name\": \"homies_final\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9143</td><td>application_1589299642358_3708</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3708/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3708_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To save the computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess, pickle\n",
    "\n",
    "def run_cmd(args_list):\n",
    "    \"\"\"Run linux commands.\"\"\"\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "    proc = subprocess.Popen(args_list, \n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stderr=subprocess.PIPE)\n",
    "    s_output, s_err = proc.communicate()\n",
    "    s_return =  proc.returncode\n",
    "    return s_return, s_output, s_err\n",
    "\n",
    "def save_hdfs(localPath, hdfsPath):\n",
    "    (ret, out, err)= run_cmd(['hdfs', 'dfs', '-put', '-f', localPath, hdfsPath])\n",
    "    if ret:\n",
    "        print(err)\n",
    "    else:\n",
    "        print('Success')\n",
    "        \n",
    "def read_hdfs(hdfsPath):\n",
    "    (ret, out, err) = run_cmd(['hdfs', 'dfs', '-cat', hdfsPath])\n",
    "    if ret:\n",
    "        print(err)\n",
    "    else:\n",
    "        print('Success')\n",
    "        return pickle.loads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the [SBB actual data](https://opentransportdata.swiss/en/dataset/istdaten) in ORC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb = spark.read.orc('/data/sbb/orc/istdaten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- betriebstag: string (nullable = true)\n",
      " |-- fahrt_bezeichner: string (nullable = true)\n",
      " |-- betreiber_id: string (nullable = true)\n",
      " |-- betreiber_abk: string (nullable = true)\n",
      " |-- betreiber_name: string (nullable = true)\n",
      " |-- produkt_id: string (nullable = true)\n",
      " |-- linien_id: string (nullable = true)\n",
      " |-- linien_text: string (nullable = true)\n",
      " |-- umlauf_id: string (nullable = true)\n",
      " |-- verkehrsmittel_text: string (nullable = true)\n",
      " |-- zusatzfahrt_tf: string (nullable = true)\n",
      " |-- faellt_aus_tf: string (nullable = true)\n",
      " |-- bpuic: string (nullable = true)\n",
      " |-- haltestellen_name: string (nullable = true)\n",
      " |-- ankunftszeit: string (nullable = true)\n",
      " |-- an_prognose: string (nullable = true)\n",
      " |-- an_prognose_status: string (nullable = true)\n",
      " |-- abfahrtszeit: string (nullable = true)\n",
      " |-- ab_prognose: string (nullable = true)\n",
      " |-- ab_prognose_status: string (nullable = true)\n",
      " |-- durchfahrt_tf: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "sbb.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the station list data [BFKOORD_GEO](https://opentransportdata.swiss/en/cookbook/hafas-rohdaten-format-hrdf/#Abgrenzung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = spark.read.csv('/data/sbb/stations/bfkoordgeo.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StationID: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      " |-- Remark: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "metadata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the latitude and longitude as float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StationID: string (nullable = true)\n",
      " |-- Longitude: float (nullable = true)\n",
      " |-- Latitude: float (nullable = true)\n",
      " |-- Height: integer (nullable = true)\n",
      " |-- Remark: float (nullable = true)"
     ]
    }
   ],
   "source": [
    "metadata = metadata.na.drop() \n",
    "\n",
    "metadata = metadata.select(\n",
    "        metadata.StationID,\n",
    "        metadata.Longitude.cast(\"float\"), \n",
    "        metadata.Latitude.cast(\"float\"), \n",
    "        metadata.Height.cast(\"int\"),\n",
    "        metadata.Remark.cast(\"float\"), \n",
    "    )\n",
    "metadata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+------+------+\n",
      "|StationID|Longitude| Latitude|Height|Remark|\n",
      "+---------+---------+---------+------+------+\n",
      "|  0000002|26.074411| 44.44677|     0|  null|\n",
      "|  0000003| 1.811446| 50.90155|     0|  null|\n",
      "|  0000004| 1.075329| 51.28421|     0|  null|\n",
      "|  0000005|-3.543547| 50.72917|     0|  null|\n",
      "|  0000007| 9.733756|46.922367|   744|  null|\n",
      "+---------+---------+---------+------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "metadata.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption 1 :** We only consider stations in a 15km radius of Zürich's train station, Zürich HB (8503000), (lat, lon) = (47.378177, 8.540192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Haversine formula to compute the distance between stations and Zurich center.(https://en.wikipedia.org/wiki/Haversine_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as functions\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "@functions.udf\n",
    "def calculateDistanceZurich(lat,lon):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6371.0\n",
    "  \n",
    "    lat = radians(lat)\n",
    "    lon = radians(lon)\n",
    "            \n",
    "    zurich_lat = radians(47.378178)\n",
    "    zurich_lon = radians(8.540192)\n",
    "    \n",
    "    hav_lat = sin((lat - zurich_lat)/2)**2\n",
    "    hav_lon = sin((lon - zurich_lon)/2)**2\n",
    "    hav = hav_lat + hav_lon * cos(lat) * cos(zurich_lat)\n",
    "    \n",
    "    d = 2*R*asin(sqrt(hav))  #distance in km\n",
    "   \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+\n",
      "|StationID|Longitude| Latitude|\n",
      "+---------+---------+---------+\n",
      "|  0000065| 8.595545| 47.40921|\n",
      "|  0000066| 8.595545| 47.40921|\n",
      "|  0000176| 8.521961| 47.35168|\n",
      "|  8502186| 8.398942|47.393406|\n",
      "|  8502187| 8.377032| 47.36474|\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "metadata_zurich = metadata.withColumn('distance_to_Zurich', calculateDistanceZurich(metadata.Latitude, metadata.Longitude))\\\n",
    "                        .filter(functions.col('distance_to_Zurich') <= 15.0)\\\n",
    "                        .select('StationID','Longitude','Latitude')\n",
    "metadata_zurich.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf\n",
    "def calculateDistance(lat1,lon1,lat2,lon2):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6371.0\n",
    "  \n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "            \n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "    \n",
    "    hav_lat = sin((lat1 - lat2)/2)**2\n",
    "    hav_lon = sin((lon1 - lon2)/2)**2\n",
    "    hav = hav_lat + hav_lon * cos(lat1) * cos(lat2)\n",
    "    \n",
    "    d = 2*R*asin(sqrt(hav))  #distance in km\n",
    "   \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pairs_distance(data):\n",
    "    \n",
    "    data_from = data.withColumnRenamed('StationID', 'from_StationID')\\\n",
    "                                      .withColumnRenamed('Longitude', 'from_Longitude')\\\n",
    "                                      .withColumnRenamed('Latitude', 'from_Latitude')\\\n",
    "                \n",
    "    data_to = data.withColumnRenamed('StationID', 'to_StationID')\\\n",
    "                                    .withColumnRenamed('Longitude', 'to_Longitude')\\\n",
    "                                    .withColumnRenamed('Latitude', 'to_Latitude')\\\n",
    "                \n",
    "    stations_pair_dist = data_from.crossJoin(data_to)\n",
    "\n",
    "    stations_pair_dist = stations_pair_dist.filter(stations_pair_dist.from_StationID != stations_pair_dist.to_StationID)\n",
    "    \n",
    "    stations_pair_dist = stations_pair_dist.withColumn('distance_km', calculateDistance(stations_pair_dist.from_Latitude, stations_pair_dist.from_Longitude,\n",
    "                                                                                stations_pair_dist.to_Latitude, stations_pair_dist.to_Longitude))\n",
    "    return stations_pair_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+------------+------------+-----------+------------------+\n",
      "|from_StationID|from_Longitude|from_Latitude|to_StationID|to_Longitude|to_Latitude|       distance_km|\n",
      "+--------------+--------------+-------------+------------+------------+-----------+------------------+\n",
      "|       0000065|      8.595545|     47.40921|     0000066|    8.595545|   47.40921|               0.0|\n",
      "|       0000065|      8.595545|     47.40921|     0000176|    8.521961|   47.35168| 8.462666257272906|\n",
      "|       0000065|      8.595545|     47.40921|     8502186|    8.398942|  47.393406|14.900964487282666|\n",
      "|       0000065|      8.595545|     47.40921|     8502187|    8.377032|   47.36474|17.177613118555456|\n",
      "|       0000065|      8.595545|     47.40921|     8502188|    8.354599|  47.355907|19.084514154615547|\n",
      "+--------------+--------------+-------------+------------+------------+-----------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "stations_pair_dist = pairs_distance(metadata_zurich)\n",
    "stations_pair_dist.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption 2** : We allow short (max 500m \"As the Crows Flies\") walking distances for transfers between two stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stations_pair_walking = stations_pair_dist.where(stations_pair_dist.distance_km <= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption 3:** We assume a walking speed of 50m/1min on a straight line, regardless of obstacles, human-built or natural, such as building, highways, rivers, or lakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+------------+------------+-----------+-------------------+-----------------+\n",
      "|from_StationID|from_Longitude|from_Latitude|to_StationID|to_Longitude|to_Latitude|        distance_km| time_walking_min|\n",
      "+--------------+--------------+-------------+------------+------------+-----------+-------------------+-----------------+\n",
      "|       0000065|      8.595545|     47.40921|     0000066|    8.595545|   47.40921|                0.0|              0.0|\n",
      "|       0000065|      8.595545|     47.40921|     8503129|    8.591911|  47.412716| 0.4761461043939778|9.522922087879556|\n",
      "|       0000065|      8.595545|     47.40921|     8587651|    8.595545|   47.40921|                0.0|              0.0|\n",
      "|       0000065|      8.595545|     47.40921|     8587655|    8.592534|   47.41272|0.45123862945729615|9.024772589145922|\n",
      "|       0000065|      8.595545|     47.40921|     8590884|    8.597269|    47.4117| 0.3058698680514894|6.117397361029788|\n",
      "+--------------+--------------+-------------+------------+------------+-----------+-------------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "stations_pair_walking = stations_pair_walking.withColumn('time_walking_min', stations_pair_walking.distance_km / 0.050)\n",
    "stations_pair_walking.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all possible pairs of stations possible for a transfer and the time needed to do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take as model a typical week from the 5th of May 2019 to 12th of May 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+-----------+-------------+\n",
      "|             trip_id|arrival_time|departure_time|    stop_id|stop_sequence|\n",
      "+--------------------+------------+--------------+-----------+-------------+\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:20:00|      04:20:00|8500010:0:3|            1|\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:24:00|      04:24:00|8500020:0:3|            2|\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:28:00|      04:28:00|8500021:0:5|            3|\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:30:00|      04:30:00|8517131:0:2|            4|\n",
      "|1.TA.1-1-B-j19-1.1.R|    04:32:00|      04:32:00|8500300:0:5|            5|\n",
      "+--------------------+------------+--------------+-----------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_stop_time = spark.read.csv('/data/sbb/timetables/csv/stop_times/2019/05/14/stop_times.txt', header=True)\n",
    "df_stop_time = df_stop_time.na.drop() \n",
    "df_stop_time = df_stop_time.select('trip_id','arrival_time','departure_time','stop_id','stop_sequence')\n",
    "df_stop_time.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the stop_id format to only get the station_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stop_time = df_stop_time.withColumn(\"stop_id\", functions.split(df_stop_time.stop_id, ':')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only keep the stops within the stations in the area of Zurich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+-------+-------------+---------+---------+---------+\n",
      "|             trip_id|arrival_time|departure_time|stop_id|stop_sequence|StationID|Longitude| Latitude|\n",
      "+--------------------+------------+--------------+-------+-------------+---------+---------+---------+\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:42:00|      02:42:00|8503305|            2|  8503305| 8.686662| 47.42581|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:46:00|      02:46:00|8503306|            3|  8503306| 8.619255|47.420197|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:50:00|      02:50:00|8503147|            4|  8503147| 8.596132|47.397213|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:55:00|      02:55:00|8503003|            5|  8503003| 8.548466| 47.36661|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:58:00|      03:00:00|8503000|            6|  8503000| 8.540192|47.378178|\n",
      "+--------------------+------------+--------------+-------+-------------+---------+---------+---------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_stop_time_zurich = df_stop_time.join(metadata_zurich, df_stop_time.stop_id == metadata_zurich.StationID, how='inner')\n",
    "df_stop_time_zurich.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove trips with hours superior to 24. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stop_time_zurich = df_stop_time_zurich.withColumn(\"arrival_hour\", functions.split(df_stop_time_zurich.arrival_time, ':')[0])\\\n",
    "                                        .filter(functions.col('arrival_hour') < 24)\\\n",
    "                                        .withColumn(\"departure_hour\", functions.split(df_stop_time_zurich.departure_time, ':')[0])\\\n",
    "                                        .filter(functions.col('departure_hour') < 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select only the traject between 8H and 17H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf\n",
    "def date_formatting(date):\n",
    "    return '2019-05-13'+' '+date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stop_time_zurich = df_stop_time_zurich.withColumn('arrival_time_format', date_formatting(df_stop_time_zurich.arrival_time))\n",
    "df_stop_time_zurich = df_stop_time_zurich.withColumn('arrival_time_format',\n",
    "                                     functions.to_timestamp(df_stop_time_zurich.arrival_time_format, 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "df_stop_time_zurich = df_stop_time_zurich.withColumn('departure_time_format', date_formatting(df_stop_time_zurich.departure_time))\n",
    "df_stop_time_zurich = df_stop_time_zurich.withColumn('departure_time_format',\n",
    "                                     functions.to_timestamp(df_stop_time_zurich.departure_time_format, 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "start_day_time = datetime.datetime.strptime('2019-05-13'+' '+'08:00:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "end_day_time = datetime.datetime.strptime('2019-05-13'+' '+'17:00:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "df_stop_time_zurich = df_stop_time_zurich.filter(df_stop_time_zurich.departure_time_format >= start_day_time)\\\n",
    "                                         .filter(df_stop_time_zurich.arrival_time_format <= end_day_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570"
     ]
    }
   ],
   "source": [
    "#Number of stations in Zurich\n",
    "df_stop_time_zurich.select('stop_id').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a list of stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f list_stations_zurich.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "#Number of stations in Zurich\n",
    "list_stations_zurich  = [station[0] for station in df_stop_time_zurich.select('stop_id').distinct().collect()] \n",
    "\n",
    "#Save the list\n",
    "with open('list_stations_zurich.pkl', 'wb') as handle:\n",
    "    pickle.dump(list_stations_zurich, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "del list_stations_zurich\n",
    "\n",
    "# send to hdfs\n",
    "save_hdfs('list_stations_zurich.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the SBB actual data and translate it from German for an easier understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbb = sbb.withColumnRenamed(\"BETRIEBSTAG\", \"trip_date\")\\\n",
    "            .withColumnRenamed(\"FAHRT_BEZEICHNER\", \"trip_id\")\\\n",
    "            .withColumnRenamed(\"BETREIBER_ID\", \"operator_id\")\\\n",
    "            .withColumnRenamed(\"BETREIBER_ABK\", \"operator_short\")\\\n",
    "            .withColumnRenamed(\"BETREIBER_NAME\", \"operator_name\")\\\n",
    "            .withColumnRenamed(\"PRODUKT_ID\", \"transport_type\")\\\n",
    "            .withColumnRenamed(\"LINIEN_ID\", \"train_id\")\\\n",
    "            .withColumnRenamed(\"LINIEN_TEXT\", \"train_type\")\\\n",
    "            .withColumnRenamed(\"UMLAUF_ID\", \"circulation_id\")\\\n",
    "            .withColumnRenamed(\"VERKEHRSMITTEL_TEXT\", \"train_text\")\\\n",
    "            .withColumnRenamed(\"ZUSATZFAHRT_TF\", \"additional_trip\")\\\n",
    "            .withColumnRenamed(\"FAELLT_AUS_TF\", \"trip_failed\")\\\n",
    "            .withColumnRenamed(\"BPUIC\", \"stop_id\")\\\n",
    "            .withColumnRenamed(\"HALTESTELLEN_NAME\", \"stop_name\")\\\n",
    "            .withColumnRenamed(\"ANKUNFTSZEIT\", \"arr_time_schedule\")\\\n",
    "            .withColumnRenamed(\"AN_PROGNOSE\", \"arr_time_actual\")\\\n",
    "            .withColumnRenamed(\"AN_PROGNOSE_STATUS\", \"arr_time_status\")\\\n",
    "            .withColumnRenamed(\"ABFAHRTSZEIT\", \"dep_time_schedule\")\\\n",
    "            .withColumnRenamed(\"AB_PROGNOSE\", \"dep_time_actual\")\\\n",
    "            .withColumnRenamed(\"AB_PROGNOSE_STATUS\", \"dep_time_status\")\\\n",
    "            .withColumnRenamed(\"DURCHFAHRT_TF\", \"stops_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep stations that are within a 15km radius around Zurich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_station_zurich = [int(row.StationID) for row in metadata_zurich.select(\"StationID\").collect()]\n",
    "df_sbb_zurich = df_sbb.where(functions.col(\"stop_id\").isin(list_station_zurich))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove rows corresponding to trips that only started, ended or went inside the radius, but the rest of the trip was outside of this radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_sbb_zurich.groupBy(\"trip_id\",\"trip_date\").count()\\\n",
    "                    .withColumnRenamed(\"trip_id\", \"trip_id_count\")\\\n",
    "                    .withColumnRenamed(\"trip_date\", \"trip_date_count\")\n",
    "\n",
    "df_sbb_zurich = df_sbb_zurich.join(counts, (df_sbb_zurich.trip_id == counts.trip_id_count) &\\\n",
    "                                   (df_sbb_zurich.trip_date == counts.trip_date_count))\\\n",
    "                            .filter(functions.col(\"count\") >1)\\\n",
    "                            .drop('trip_id_count')\\\n",
    "                            .drop('trip_date_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should first clean the SBB data before computing the delays.\n",
    "\n",
    "We only want trips that are part of the regular schedule (addional_trip == False), those which didn't fail (trip_failed == False), and those who actually stop at the indicated station (stops_here == False).\n",
    "\n",
    "For the arrival and departure time status, there are 4 possible values: REAL (real), PROGNOSE (forecast), GESCHAETZT (estimated) and UNBEKANNT (unknown). We remove all the rows corresponding to unknown status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbb_zurich = df_sbb_zurich.filter(functions.col(\"additional_trip\") == False)\\\n",
    "                            .filter(functions.col(\"trip_failed\") == False)\\\n",
    "                            .filter(functions.col(\"stops_here\") == False)\\\n",
    "                            .filter(functions.col(\"dep_time_status\") != 'UNBEKANNT')\\\n",
    "                            .filter(functions.col(\"arr_time_status\") != 'UNBEKANNT')\\\n",
    "                            .drop(\"additional_trip\")\\\n",
    "                            .drop(\"trip_failed\")\\\n",
    "                            .drop(\"stops_here\")\n",
    "\n",
    "df_sbb_zurich.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the delays between the scheduled and actual times for departure and arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeFormatSchedule = \"dd.MM.yyyy HH:mm\"\n",
    "timeFormatActual = \"dd.MM.yyyy HH:mm:ss\"\n",
    "\n",
    "departure_delay = (functions.unix_timestamp('dep_time_schedule', format=timeFormatSchedule)\\\n",
    "                   - functions.unix_timestamp('dep_time_actual', format=timeFormatActual))\n",
    "arrival_delay = (functions.unix_timestamp('arr_time_schedule', format=timeFormatSchedule)\\\n",
    "                 - functions.unix_timestamp('arr_time_actual', format=timeFormatActual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some more filtering on the trip times and dates. We first only keep trips that happened during the weekdays (Monday to Friday). Then we're only interested in \"reasonable hours\" of the day. We thus decided to remove trips with arrival times before 8 a.m and after 5 p.m. Finally, we only take into account trips that happened in 2019. This will help reducing the amount of data and thus the computation time.\n",
    "\n",
    "We also make sure to remove all the Null entries for arrival and departure times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay = df_sbb_zurich.withColumn(\"departure_delay\", functions.when(departure_delay>0,departure_delay).otherwise(0))\\\n",
    "                        .withColumn(\"arrival_delay\", functions.when(arrival_delay>0,arrival_delay).otherwise(0))\\\n",
    "                        .withColumn(\"dep_time_schedule\",functions.to_timestamp(df_sbb_zurich.dep_time_schedule, 'dd.MM.yyyy HH:mm'))\\\n",
    "                        .withColumn(\"arr_time_schedule\",functions.to_timestamp(df_sbb_zurich.arr_time_schedule, 'dd.MM.yyyy HH:mm'))\\\n",
    "                        .withColumn(\"dep_time_actual\",functions.to_timestamp(df_sbb_zurich.dep_time_actual, 'dd.MM.yyyy HH:mm:ss'))\\\n",
    "                        .withColumn(\"arr_time_actual\",functions.to_timestamp(df_sbb_zurich.arr_time_actual, 'dd.MM.yyyy HH:mm:ss'))\\\n",
    "                        .filter(functions.year('arr_time_schedule')==2019)\\\n",
    "                        .filter(functions.date_format('arr_time_schedule', 'u')<=5)\\\n",
    "                        .filter(functions.hour(functions.col(\"arr_time_schedule\")).cast(\"int\") >= 8)\\\n",
    "                        .filter(functions.hour(functions.col(\"arr_time_schedule\")).cast(\"int\") <= 17)\\\n",
    "                        .where(functions.col(\"dep_time_schedule\").isNotNull())\\\n",
    "                        .where(functions.col(\"arr_time_schedule\").isNotNull())\\\n",
    "                        .where(functions.col(\"dep_time_actual\").isNotNull())\\\n",
    "                        .where(functions.col(\"arr_time_actual\").isNotNull())\n",
    "\n",
    "df_delay.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the delays for each trip, we can start building the hole SBB schedule. It will contain the departure and arrival stations, with the corresponding departure and arrival schedule times, and the delay of the actual arrival time (with the respect to the scheduled one). It will also contain the trip ID corresponding to the trip between the stations.\n",
    "\n",
    "In order to build this schedule, we need to reconstruct the journey of each trip. We thus group the data we have by the trip information we have (id and date), then order it by ascending scheduled departure time. We will thus have for each trip id and date the stations of the journey in order. We use this order to build a metric corresponding to the row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_trip_id = Window.partitionBy(\"trip_id\",\"trip_date\").orderBy(\"dep_time_schedule\")\n",
    "df_order_trips = df_delay.withColumn(\"order\", functions.row_number().over(window_trip_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all stations of all trips are in order and have their corresponding order, we can create two dataframes based on the previously built one (one for the departure station and one for the arrival station). Joining both of them will give us the whole schedule, with the actual arrival delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_trips_prev = df_order_trips.select('trip_id', 'trip_date', 'stop_id', 'order', 'dep_time_schedule', 'arr_time_schedule')\\\n",
    "                                    .withColumnRenamed(\"stop_id\", \"stop_id_prev\")\\\n",
    "                                    .withColumnRenamed(\"arr_time_schedule\", \"arrival_prev\")\\\n",
    "                                    .withColumnRenamed(\"dep_time_schedule\", \"departure_prev\")\\\n",
    "                                    .select(\"trip_id\",'trip_date',\"stop_id_prev\",\"arrival_prev\",'departure_prev','order')\\\n",
    "                                    .withColumn('next_order_prev', (df_order_trips.order.cast('int') + 1 ).cast('string'))\n",
    "\n",
    "df_trips_next = df_order_trips.select('trip_id', 'trip_date', 'stop_id', 'order', 'dep_time_schedule', 'arr_time_schedule', 'arrival_delay')\\\n",
    "                                    .withColumnRenamed(\"trip_id\", \"trip_id_next\")\\\n",
    "                                    .withColumnRenamed(\"trip_date\", \"trip_date_next\")\\\n",
    "                                    .withColumnRenamed(\"stop_id\", \"stop_id_next\")\\\n",
    "                                    .withColumnRenamed(\"arr_time_schedule\", \"arrival_next\")\\\n",
    "                                    .withColumnRenamed(\"dep_time_schedule\", \"departure_next\")\\\n",
    "                                    .withColumnRenamed(\"order\", \"order_next\")\n",
    "\n",
    "df_trips_schedule = df_trips_prev.join(df_trips_next,\n",
    "                                       (df_trips_prev.trip_id == df_trips_next.trip_id_next) &\\\n",
    "                                       (df_trips_prev.trip_date == df_trips_next.trip_date_next) &\\\n",
    "                                       (df_trips_prev.next_order_prev == df_trips_next.order_next),\n",
    "                                       how='inner')\\\n",
    "                                .select('trip_id', 'trip_date', 'stop_id_prev', 'stop_id_next',\n",
    "                                        'arrival_prev', 'arrival_next', 'arrival_delay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to group the dataframe in order to have an idea on the average delay between two given stations.\n",
    "\n",
    "We have to format the arrival and departure times to remove the date since we want only want to group on the hour of the trip. This means that we consider that all trips between two stations happening at the same hour of the day (for example between 10:00 and 10:59) as equivalent, also that different days for a same exact trip are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_trips_schedule = df_trips_schedule.withColumn(\"arrival_next_time\",\n",
    "                                                 functions.split(df_trips_schedule.arrival_next, ' ')[1])\\\n",
    "                                    .withColumn(\"arrival_prev_time\",\n",
    "                                                functions.split(df_trips_schedule.arrival_prev, ' ')[1])\n",
    "df_trips_schedule = df_trips_schedule.withColumn(\"arrival_next_hour\",\n",
    "                                                functions.hour(df_trips_schedule.arrival_next_time))\\\n",
    "                                    .withColumn(\"arrival_prev_hour\",\n",
    "                                                functions.hour(df_trips_schedule.arrival_prev_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_delay_dist = df_trips_schedule.groupBy(['stop_id_prev','stop_id_next',\n",
    "                                           'arrival_next_hour','arrival_prev_hour']).agg(functions.mean('arrival_delay').alias('med_val'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert the dataframe to Pandas and save it in order to have an easier access later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd_delay_dist = df_delay_dist.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd_delay_dist.to_pickle('pd_delay_dist1yearmeanperh.pkl')\n",
    "save_hdfs('pd_delay_dist1yearmeanperh.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contruction of the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will contsruct a graph where the nodes are the events that happen on a typical week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 possible types of connections: the natural one that is the mean of transport connects the two stations and the one where the traveler makes a transfer by changing the mean of transport or  walking a maximum distance of 500m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct this big graph we will need to gather some information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct 2 adjency matrices for the 2 possibilites with 1 at entry (i,j) if there exists a connection between station i and j and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  StationID\n",
      "0   8589111\n",
      "1   8591190\n",
      "2   8503078\n",
      "3   8591284\n",
      "4   8503088"
     ]
    }
   ],
   "source": [
    "df_station_id = df_stop_time_zurich.select('StationID').distinct().toPandas()\n",
    "df_station_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570"
     ]
    }
   ],
   "source": [
    "len(df_station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           index\n",
      "StationID       \n",
      "8589111        0\n",
      "8591190        1\n",
      "8503078        2\n",
      "8591284        3\n",
      "8503088        4"
     ]
    }
   ],
   "source": [
    "df_index = df_station_id.reset_index().set_index('StationID')\n",
    "df_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f df_index.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "# save to pickle\n",
    "df_index.to_pickle('df_index.pkl')\n",
    "save_hdfs('df_index.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'8589111'"
     ]
    }
   ],
   "source": [
    "df_station_id.loc[0,'StationID']   #put index gives stationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "df_index.loc['8591284','index']   #put stationID gives index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to go from an index to a station_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|stop_id|stop_id_next|\n",
      "+-------+------------+\n",
      "|8590787|     8590799|\n",
      "|8573712|     8503693|\n",
      "|8591118|     8591160|\n",
      "|8587420|     8576152|\n",
      "|8573205|     8588553|\n",
      "|8590471|     8591278|\n",
      "|8583761|     8582662|\n",
      "|8590752|     8590610|\n",
      "|8591378|     8590684|\n",
      "|8503053|     8503054|\n",
      "|8591281|     8591046|\n",
      "|8590773|     8590771|\n",
      "|8583731|     8582752|\n",
      "|8594249|     8590752|\n",
      "|8502955|     8572646|\n",
      "|8503010|     8503202|\n",
      "|8590632|     8588745|\n",
      "|8591061|     8591270|\n",
      "|8591439|     8591106|\n",
      "|8591053|     8591307|\n",
      "+-------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_stop_time_zurich_prev = df_stop_time_zurich.select('trip_id','stop_id','stop_sequence')\\\n",
    "                  .withColumn('next_stop_sequence_prev', (df_stop_time_zurich.stop_sequence.cast('int') + 1 ).cast('string'))\n",
    "\n",
    "df_stop_time_zurich_next = df_stop_time_zurich.withColumnRenamed(\"trip_id\", \"trip_id_next\")\\\n",
    "                                              .withColumnRenamed(\"stop_id\", \"stop_id_next\")\\\n",
    "                                              .withColumnRenamed(\"stop_sequence\", \"stop_sequence_next\")\\\n",
    "                                              .select(\"trip_id_next\",\"stop_id_next\",\"stop_sequence_next\")\n",
    "\n",
    "df_pairs = df_stop_time_zurich_prev.join(df_stop_time_zurich_next, \n",
    "                                         (df_stop_time_zurich_prev.trip_id == df_stop_time_zurich_next.trip_id_next) &\\\n",
    "                                         (df_stop_time_zurich_prev.next_stop_sequence_prev == df_stop_time_zurich_next.stop_sequence_next),\n",
    "                                          how='inner')\\\n",
    "                                    .select('stop_id','stop_id_next').distinct()\n",
    "#all possible direct edges\n",
    "df_pairs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stop_id stop_id_next\n",
      "0  8590787      8590799\n",
      "1  8573712      8503693\n",
      "2  8591118      8591160\n",
      "3  8587420      8576152\n",
      "4  8573205      8588553"
     ]
    }
   ],
   "source": [
    "df_pairs = df_pairs.toPandas()\n",
    "df_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "trans_matrix = np.zeros((len(df_station_id),len(df_station_id)))\n",
    "\n",
    "for index, row in df_pairs.iterrows():\n",
    "    index1 = df_index.loc[row['stop_id'],'index']\n",
    "    index2 = df_index.loc[row['stop_id_next'],'index']\n",
    "    trans_matrix[index1,index2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to construct the walking adjency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+------------+------------+-----------+--------------------+-------------------+\n",
      "|from_StationID|from_Longitude|from_Latitude|to_StationID|to_Longitude|to_Latitude|         distance_km|   time_walking_min|\n",
      "+--------------+--------------+-------------+------------+------------+-----------+--------------------+-------------------+\n",
      "|       8503077|      8.596796|    47.347317|     8503078|    8.593024|  47.345474|  0.3503261531304608|  7.006523062609215|\n",
      "|       8591903|      8.596038|     47.34835|     8503078|    8.593024|  47.345474|  0.3922255506454547|  7.844511012909094|\n",
      "|       8591023|      8.598149|     47.34682|     8503078|    8.593024|  47.345474| 0.41414706143344815|  8.282941228668962|\n",
      "|       8590879|        8.5933|     47.34539|     8503078|    8.593024|  47.345474|0.022765925906543028|0.45531851813086055|\n",
      "|       8576189|      8.588489|      47.3457|     8503078|    8.593024|  47.345474| 0.34264594945678645|  6.852918989135729|\n",
      "+--------------+--------------+-------------+------------+------------+-----------+--------------------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#this double join give us information about the two walking stations\n",
    "stations_stop_pair_walking = df_stop_time_zurich.select('StationID').distinct().join(stations_pair_walking, \n",
    "                                                        df_stop_time_zurich.StationID == stations_pair_walking.from_StationID,\n",
    "                                                                                    how = 'inner').drop('StationID')\n",
    "\n",
    "stations_stop_pair_walking = df_stop_time_zurich.select('StationID').distinct().join(stations_stop_pair_walking, \n",
    "                                                        df_stop_time_zurich.StationID == stations_pair_walking.to_StationID,\n",
    "                                                                                    how = 'inner').drop('StationID')\n",
    "stations_stop_pair_walking.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  from_StationID to_StationID\n",
      "0        8590879      8503078\n",
      "1        8503077      8503078\n",
      "2        8591903      8503078\n",
      "3        8591023      8503078\n",
      "4        8576189      8503078"
     ]
    }
   ],
   "source": [
    "walking_matrix = np.zeros((len(df_station_id),len(df_station_id)))\n",
    "\n",
    "#dict to find the walking time easily \n",
    "#a minimum of 2 minutes for a transfer\n",
    "dict_walk_time = { (r['from_StationID'], r['to_StationID']) : (r['time_walking_min'] + 2 )for r in stations_stop_pair_walking.collect()}\n",
    "\n",
    "#collect the pairs to fill the matrix \n",
    "df_walking_pair_station = stations_stop_pair_walking.select('from_StationID','to_StationID').distinct().toPandas()\n",
    "df_walking_pair_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the dictionnary of the walking time between stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f dict_walk_time.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "#Save the dictionnary \n",
    "with open('dict_walk_time.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_walk_time, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "del dict_walk_time\n",
    "\n",
    "# send to hdfs\n",
    "save_hdfs('dict_walk_time.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a walking matrix and dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index, row in df_walking_pair_station.iterrows():\n",
    "    index1 = df_index.loc[row['from_StationID'], 'index']\n",
    "    index2 = df_index.loc[row['to_StationID'], 'index']\n",
    "    walking_matrix[index1,index2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "walking_neighbors={}\n",
    "\n",
    "for i in range(len(walking_matrix)):\n",
    "    key = df_station_id.loc[i,'StationID']\n",
    "    values = set()\n",
    "    for j in range(len(walking_matrix)):\n",
    "        if (walking_matrix[i,j]):\n",
    "            values.add(df_station_id.loc[j,'StationID'])\n",
    "    walking_neighbors[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f dict_walking_neighbors.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "#Save the dictionnary \n",
    "with open('dict_walking_neighbors.pkl', 'wb') as handle:\n",
    "    pickle.dump(walking_neighbors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "del walking_neighbors\n",
    "\n",
    "# send to hdfs\n",
    "save_hdfs('dict_walking_neighbors.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge the 2 adjency matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 1, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])"
     ]
    }
   ],
   "source": [
    "adj_matrix = np.logical_or(trans_matrix, walking_matrix).astype(int)\n",
    "adj_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute new Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the information to compute the edges and the vertices of the graph. We can build a schedule using the stop_sequence of each station and the associated trip_id to know which station comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+------------+--------------+------------+--------------+\n",
      "|             trip_id|stop_id_prev|stop_id_next|arrival_prev|departure_prev|arrival_next|departure_next|\n",
      "+--------------------+------------+------------+------------+--------------+------------+--------------+\n",
      "|1.TA.26-10-j19-1.1.H|     8590620|     8590626|    15:17:00|      15:17:00|    15:18:00|      15:18:00|\n",
      "|1.TA.26-161-j19-1...|     8591241|     8591080|    12:33:00|      12:33:00|    12:33:00|      12:33:00|\n",
      "|1.TA.26-38-j19-1.1.R|     8591045|     8591043|    12:14:00|      12:14:00|    12:14:00|      12:14:00|\n",
      "|1.TA.26-760-j19-1...|     8596113|     8591065|    16:23:00|      16:23:00|    16:25:00|      16:25:00|\n",
      "|10.TA.26-14-A-j19...|     8591193|     8591276|    09:03:00|      09:03:00|    09:04:00|      09:05:00|\n",
      "+--------------------+------------+------------+------------+--------------+------------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_stop_time_prev = df_stop_time_zurich.select('trip_id', 'stop_id', 'stop_sequence', 'arrival_time', 'departure_time')\\\n",
    "                                    .withColumnRenamed(\"stop_id\", \"stop_id_prev\")\\\n",
    "                                    .withColumnRenamed(\"arrival_time\", \"arrival_prev\")\\\n",
    "                                    .withColumnRenamed(\"departure_time\", \"departure_prev\")\\\n",
    "                                    .select(\"trip_id\",\"stop_id_prev\",\"arrival_prev\",'departure_prev','stop_sequence')\\\n",
    "                                    .withColumn('next_stop_sequence_prev', (df_stop_time_zurich.stop_sequence.cast('int') + 1 ).cast('string'))\n",
    "\n",
    "df_stop_time_next = df_stop_time_zurich.select('trip_id', 'stop_id', 'stop_sequence', 'arrival_time', 'departure_time')\\\n",
    "                                    .withColumnRenamed(\"trip_id\", \"trip_id_next\")\\\n",
    "                                    .withColumnRenamed(\"stop_id\", \"stop_id_next\")\\\n",
    "                                    .withColumnRenamed(\"arrival_time\", \"arrival_next\")\\\n",
    "                                    .withColumnRenamed(\"departure_time\", \"departure_next\")\\\n",
    "                                    .withColumnRenamed(\"stop_sequence\", \"stop_sequence_next\")\\\n",
    "                                    .select(\"trip_id_next\",\"stop_id_next\",\"arrival_next\",'departure_next','stop_sequence_next')\n",
    "\n",
    "df_schedule = df_stop_time_prev.join(df_stop_time_next,\n",
    "                                     (df_stop_time_prev.trip_id == df_stop_time_next.trip_id_next) &\\\n",
    "                                     (df_stop_time_prev.next_stop_sequence_prev == df_stop_time_next.stop_sequence_next),\n",
    "                                     how='inner')\\\n",
    "                                .select('trip_id', 'stop_id_prev', 'stop_id_next', 'arrival_prev', 'departure_prev', 'arrival_next', 'departure_next')\n",
    "df_schedule.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the possible connections of this week which constitues some of the graph direct edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953542"
     ]
    }
   ],
   "source": [
    "#number of direct edges\n",
    "df_schedule.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns with dates transformed in time stamp to ease the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_schedule = df_schedule.withColumn('arrival_prev_time', date_formatting(df_schedule.arrival_prev))\n",
    "df_schedule = df_schedule.withColumn('arrival_prev_time',\n",
    "                                     functions.to_timestamp(df_schedule.arrival_prev_time, 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "df_schedule = df_schedule.withColumn('departure_prev_time', date_formatting(df_schedule.departure_prev))\n",
    "df_schedule = df_schedule.withColumn('departure_prev_time',\n",
    "                                     functions.to_timestamp(df_schedule.departure_prev_time, 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "df_schedule = df_schedule.withColumn('arrival_next_time', date_formatting(df_schedule.arrival_next))\n",
    "df_schedule = df_schedule.withColumn('arrival_next_time',\n",
    "                                     functions.to_timestamp(df_schedule.arrival_next_time, 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "df_schedule = df_schedule.withColumn('departure_next_time', date_formatting(df_schedule.departure_next))\n",
    "df_schedule = df_schedule.withColumn('departure_next_time',\n",
    "                                     functions.to_timestamp(df_schedule.departure_next_time, 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf\n",
    "def create_id(stop_id,time):\n",
    "    return stop_id+'_'+time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf\n",
    "def compute_time(departure_time, arr_time):\n",
    "    return (departure_time - arr_time).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by computing all the simple edges with the weights being the time needed to go from src (source) to dst (destination). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_edges = df_schedule.withColumn('src',\n",
    "                       create_id(functions.col('stop_id_prev'), functions.col('arrival_prev')))\\\n",
    "            .withColumn('dst',\n",
    "                       create_id(functions.col('stop_id_next'), functions.col('arrival_next')))\\\n",
    "            .withColumn('weight',\n",
    "                        compute_time (functions.col('arrival_next_time'), functions.col('arrival_prev_time')) ) \\\n",
    "            .select('src','dst','weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_edges = df_edges.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------+\n",
      "|             src|             dst|weight|\n",
      "+----------------+----------------+------+\n",
      "|8590620_15:17:00|8590626_15:18:00|  60.0|\n",
      "|8591241_12:33:00|8591080_12:33:00|   0.0|\n",
      "|8591045_12:14:00|8591043_12:14:00|   0.0|\n",
      "|8596113_16:23:00|8591065_16:25:00| 120.0|\n",
      "|8591193_09:03:00|8591276_09:04:00|  60.0|\n",
      "|8573164_11:49:00|8573165_11:50:00|  60.0|\n",
      "|8590544_15:33:00|8590542_15:34:00|  60.0|\n",
      "|8502776_10:46:00|8573163_10:47:00|  60.0|\n",
      "|8591196_16:12:00|8591230_16:14:00| 120.0|\n",
      "|8503378_14:27:00|8588279_14:29:00| 120.0|\n",
      "|8591832_12:40:00|8591147_12:41:00|  60.0|\n",
      "|8590610_12:28:00|8594249_12:30:00| 120.0|\n",
      "|8591258_08:02:00|8591311_08:03:00|  60.0|\n",
      "|8590707_16:58:00|8590709_16:59:00|  60.0|\n",
      "|8591174_14:39:00|8588078_14:41:00| 120.0|\n",
      "|8591329_11:43:00|8591366_11:45:00| 120.0|\n",
      "|8591410_14:07:00|8591268_14:08:00|  60.0|\n",
      "|8589110_08:19:00|8589109_08:20:00|  60.0|\n",
      "|8581015_14:06:00|8573178_14:09:00| 180.0|\n",
      "|8580912_14:24:00|8503610_14:27:00| 180.0|\n",
      "+----------------+----------------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_edges.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f dict_edges_simple.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "pre_dict_edges_simple = df_edges.groupby('dst').agg(functions.collect_list('src'))\n",
    "dict_edges_simple = { r['dst'] : r['collect_list(src)']  for r in pre_dict_edges_simple.collect()}\n",
    "\n",
    "#Save the dictionnary \n",
    "with open('dict_edges_simple.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_edges_simple, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "del dict_edges_simple\n",
    "\n",
    "# send to hdfs\n",
    "save_hdfs('dict_edges_simple.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339261"
     ]
    }
   ],
   "source": [
    "df_edges.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_vertices = df_edges.select('src').withColumnRenamed('src', 'id')\\\n",
    "                                    .union(df_edges.select('dst').withColumnRenamed('src', 'id'))\\\n",
    "                                    .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|              id|\n",
      "+----------------+\n",
      "|8572598_16:47:00|\n",
      "|8576248_16:25:00|\n",
      "|8590716_08:45:00|\n",
      "|8591100_11:30:00|\n",
      "|8590318_16:33:00|\n",
      "|8503579_10:32:00|\n",
      "|8591051_08:37:00|\n",
      "|8591379_16:39:00|\n",
      "|8590587_11:02:00|\n",
      "|8590554_16:26:00|\n",
      "|8591219_15:51:00|\n",
      "|8591101_08:20:00|\n",
      "|8502572_09:44:00|\n",
      "|8590552_15:03:00|\n",
      "|8591273_12:28:00|\n",
      "|8580301_14:36:00|\n",
      "|8591079_08:12:00|\n",
      "|8591067_16:46:00|\n",
      "|8594261_14:36:00|\n",
      "|8503378_11:30:00|\n",
      "+----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_vertices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229314"
     ]
    }
   ],
   "source": [
    "df_vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save\n",
    "#df_vertices.write.format(\"orc\").save(\"/user/{}/df_vertices_bis.orc\".format(\"kooli\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f pd_vertices.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "pd_vertices = df_vertices.toPandas()\n",
    "pd_vertices.to_pickle('pd_vertices.pkl')\n",
    "save_hdfs('pd_vertices.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now that we have all the nodes and the direct edges, we need to add the possibilities to make a transfer, by stoping at a stop and taking another mean of transport from the same stop or another stop nearby. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list for each station all the scheduled stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------+\n",
      "|stop_id|collect_list(arrival_time)|\n",
      "+-------+--------------------------+\n",
      "|8503078|      [15:50:00, 16:05:...|\n",
      "|8503088|      [08:15:00, 08:25:...|\n",
      "|8573729|      [16:39:00, 16:09:...|\n",
      "|8589111|      [13:45:00, 13:45:...|\n",
      "|8590819|      [15:45:00, 15:15:...|\n",
      "|8591190|      [15:13:00, 09:43:...|\n",
      "|8503376|      [08:03:00, 09:50:...|\n",
      "|8506895|      [16:12:00, 16:12:...|\n",
      "|8587967|      [16:09:00, 16:17:...|\n",
      "|8591284|      [15:45:00, 15:15:...|\n",
      "|8590478|      [13:38:00, 13:18:...|\n",
      "|8590804|      [09:56:00, 09:41:...|\n",
      "|8591315|      [14:25:00, 14:19:...|\n",
      "|8591362|      [09:37:00, 16:13:...|\n",
      "|8588312|      [16:48:00, 16:48:...|\n",
      "|8590541|      [09:38:00, 09:08:...|\n",
      "|8591149|      [08:58:00, 08:50:...|\n",
      "|8590273|      [16:51:00, 16:36:...|\n",
      "|8590477|      [13:29:00, 12:59:...|\n",
      "|8591053|      [14:16:00, 08:56:...|\n",
      "+-------+--------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_stop_list_hours = df_stop_time_zurich.groupby('stop_id').agg(functions.collect_list('arrival_time'))\n",
    "df_stop_list_hours.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Connect all nodes within the same stop_id and having at least 2 minutes of interval \n",
    "#Returns for each stop_id a list of edges to add \n",
    "@functions.udf('array<array<string>>')\n",
    "def add_edges_correspondance(stop_id, list_hours):\n",
    "    edges = []\n",
    "    for hour_before, hour_after in zip(list_hours[:-1], list_hours[1:]):\n",
    "        date1 = '2019-05-13'+' '+hour_before\n",
    "        date2 = '2019-05-13'+' '+hour_after\n",
    "        \n",
    "        time_before = datetime.datetime.strptime(date1, \"%Y-%m-%d %H:%M:%S\")\n",
    "        time_after =  datetime.datetime.strptime(date2, \"%Y-%m-%d %H:%M:%S\")\n",
    "        transfer_time = (time_after - time_before).total_seconds()\n",
    "        \n",
    "        if( transfer_time >= 120 ):  #only make a transfer if he has 2 minutes ?\n",
    "            e = [stop_id+'_'+hour_before, stop_id+'_'+hour_after, transfer_time]\n",
    "            edges.append(e)  \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------+--------------------+\n",
      "|stop_id|collect_list(arrival_time)|               edges|\n",
      "+-------+--------------------------+--------------------+\n",
      "|8503078|      [15:50:00, 16:05:...|[[8503078_15:50:0...|\n",
      "|8503088|      [08:15:00, 08:25:...|[[8503088_08:15:0...|\n",
      "|8573729|      [16:39:00, 16:09:...|[[8573729_16:09:0...|\n",
      "|8589111|      [13:45:00, 13:45:...|[[8589111_08:15:0...|\n",
      "|8590819|      [15:45:00, 15:15:...|[[8590819_10:45:0...|\n",
      "|8591190|      [15:13:00, 09:43:...|[[8591190_09:33:0...|\n",
      "|8503376|      [08:03:00, 09:50:...|[[8503376_08:03:0...|\n",
      "|8506895|      [16:12:00, 16:12:...|[[8506895_08:12:0...|\n",
      "|8587967|      [16:09:00, 16:17:...|[[8587967_16:09:0...|\n",
      "|8591284|      [15:45:00, 15:15:...|[[8591284_08:45:0...|\n",
      "|8590478|      [13:38:00, 13:18:...|[[8590478_12:18:0...|\n",
      "|8590804|      [09:56:00, 09:41:...|[[8590804_08:11:0...|\n",
      "|8591315|      [14:25:00, 14:19:...|[[8591315_12:55:0...|\n",
      "|8591362|      [09:37:00, 16:13:...|[[8591362_09:37:0...|\n",
      "|8588312|      [16:48:00, 16:48:...|[[8588312_08:58:0...|\n",
      "|8590541|      [09:38:00, 09:08:...|[[8590541_08:38:0...|\n",
      "|8591149|      [08:58:00, 08:50:...|[[8591149_08:50:0...|\n",
      "|8503772|      [16:48:00, 16:48:...|[[8503772_08:48:0...|\n",
      "|8580432|      [10:52:00, 10:22:...|[[8580432_08:22:0...|\n",
      "|8591085|      [08:47:00, 08:17:...|[[8591085_08:17:0...|\n",
      "+-------+--------------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_stop_list_hours = df_stop_list_hours.withColumn('edges',\n",
    "                              add_edges_correspondance(functions.col('stop_id'), \n",
    "                                                      functions.col('collect_list(arrival_time)')) )\n",
    "df_stop_list_hours.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each stop we have a list of possible transfers that can be made by stopping at a station and take a different mean of transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f dict_stop_list_hours.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "dict_stop_list_hours = { r['stop_id'] : r['collect_list(arrival_time)']  for r in df_stop_list_hours.collect()}\n",
    "\n",
    "#Save the dictionnary \n",
    "with open('dict_stop_list_hours.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_stop_list_hours, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "del dict_stop_list_hours\n",
    "\n",
    "# send to hdfs\n",
    "save_hdfs('dict_stop_list_hours.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_edges_array = df_stop_list_hours.withColumn(\"col\", functions.explode(\"edges\")).select('col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------+\n",
      "|             src|             dst|weight|\n",
      "+----------------+----------------+------+\n",
      "|8503088_08:31:00|8503088_08:51:00|1200.0|\n",
      "|8503088_08:51:00|8503088_09:31:00|2400.0|\n",
      "|8503088_09:31:00|8503088_09:51:00|1200.0|\n",
      "|8503088_09:51:00|8503088_10:31:00|2400.0|\n",
      "|8503088_10:31:00|8503088_10:51:00|1200.0|\n",
      "|8503088_10:51:00|8503088_11:31:00|2400.0|\n",
      "|8503088_11:31:00|8503088_11:51:00|1200.0|\n",
      "|8503088_11:51:00|8503088_12:31:00|2400.0|\n",
      "|8503088_12:31:00|8503088_12:51:00|1200.0|\n",
      "|8503088_12:51:00|8503088_13:31:00|2400.0|\n",
      "|8503088_13:31:00|8503088_13:51:00|1200.0|\n",
      "|8503088_13:51:00|8503088_14:31:00|2400.0|\n",
      "|8503088_14:31:00|8503088_14:51:00|1200.0|\n",
      "|8503088_14:51:00|8503088_15:31:00|2400.0|\n",
      "|8503088_15:31:00|8503088_15:51:00|1200.0|\n",
      "|8503088_15:51:00|8503088_16:31:00|2400.0|\n",
      "|8503088_16:31:00|8503088_16:51:00|1200.0|\n",
      "|8503088_08:38:00|8503088_08:58:00|1200.0|\n",
      "|8503088_08:58:00|8503088_09:38:00|2400.0|\n",
      "|8503088_09:38:00|8503088_09:58:00|1200.0|\n",
      "+----------------+----------------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_edges_array = df_edges_array.withColumn('src', df_edges_array.col.getItem(0))\n",
    "df_edges_array = df_edges_array.withColumn('dst', df_edges_array.col.getItem(1)) \n",
    "df_edges_array = df_edges_array.withColumn('weight', df_edges_array.col.getItem(2))\n",
    "df_edges_correspondance =  df_edges_array.drop('col')\n",
    "df_edges_correspondance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f dict_edges_corr.pkl /user/kooli/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "pre_dict_edges_corr = df_edges_correspondance.groupby('dst').agg(functions.collect_list('src'))\n",
    "dict_edges_corr = { r['dst'] : r['collect_list(src)']  for r in pre_dict_edges_corr.collect()}\n",
    "\n",
    "#Save the dictionnary \n",
    "with open('dict_edges_corr.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_edges_corr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "del dict_edges_corr\n",
    "\n",
    "# send to hdfs\n",
    "save_hdfs('dict_edges_corr.pkl', '/user/{}/'.format('kooli'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243224"
     ]
    }
   ],
   "source": [
    "df_edges_correspondance.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save the correspondance edges to avoid recomputing them \n",
    "#df_edges_correspondance.write.csv(\"/user/chahed/edges_corr_filtered_final_corrected.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now focus on the transfer that can be made by walking to a nearby station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf('array<array<string>>')\n",
    "def add_edges_walking(node_id):\n",
    "    stop_id, time = node_id.split('_')\n",
    "    index_node = df_index.loc[stop_id,'index'] \n",
    "    edges = []\n",
    "    \n",
    "    for i in range(len(df_index)):  #iterate over the other nodes \n",
    "        if(walking_matrix[index_node,i]): #if it's possible to go by walking from the present station\n",
    "            \n",
    "            station_id_other = df_station_id.loc[i,'StationID']\n",
    "            \n",
    "            walking_time = datetime.timedelta(minutes = dict_walk_time[(stop_id, station_id_other)] )\n",
    "            time_formatted = datetime.datetime.strptime('2019-05-13'+' '+time, \"%Y-%m-%d %H:%M:%S\")\n",
    "            arrival_time = time_formatted + walking_time\n",
    "            \n",
    "            hours_other = dict_stop_list_hours[station_id_other]\n",
    "            possible_changes = filter( (lambda x : x > arrival_time) , \\\n",
    "                                    map(lambda x: datetime.datetime.strptime('2019-05-13'+' '+x, \"%Y-%m-%d %H:%M:%S\"), \\\n",
    "                                        hours_other) )\n",
    "            \n",
    "            if(len(possible_changes) != 0):\n",
    "                first_hour_change = min(possible_changes)\n",
    "                                    \n",
    "                #destination node\n",
    "                first_hour_change_str = first_hour_change.strftime('%H:%M:%S')\n",
    "                node_id_other = station_id_other+'_'+first_hour_change_str\n",
    "            \n",
    "                time_transfer = (first_hour_change - time_formatted).total_seconds()\n",
    "            \n",
    "                edges.append([node_id, node_id_other, time_transfer])\n",
    "            \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_edges_correspondance_walking = df_vertices.withColumn('edges',add_edges_walking(functions.col('id')) )\n",
    "df_edges_correspondance_walking.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_edges_correspondance_walking_exploded = df_edges_correspondance_walking.withColumn(\"edges\", \n",
    "                                                                                      functions.explode(\"edges\")).select('edges')\n",
    "df_edges_correspondance_walking_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------+\n",
      "|             src|             dst|weight|\n",
      "+----------------+----------------+------+\n",
      "|8588078_15:29:00|8503088_15:47:00|1080.0|\n",
      "|8588078_15:29:00|8591327_15:37:00| 480.0|\n",
      "|8588078_15:29:00|8591123_15:39:00| 600.0|\n",
      "|8588078_15:29:00|8587349_15:35:00| 360.0|\n",
      "|8588078_15:29:00|8591067_15:38:00| 540.0|\n",
      "|8588078_15:29:00|8503446_16:00:00|1860.0|\n",
      "|8588078_15:29:00|8530822_15:41:00| 720.0|\n",
      "|8588078_15:29:00|8591174_15:38:00| 540.0|\n",
      "|8588078_15:29:00|8591379_15:39:00| 600.0|\n",
      "|8588078_15:29:00|8503000_15:38:00| 540.0|\n",
      "|8588078_15:29:00|8587348_15:38:00| 540.0|\n",
      "|8588078_15:29:00|8503500_15:35:00| 360.0|\n",
      "|8588078_15:29:00|8503499_15:33:00| 240.0|\n",
      "|8591119_16:50:00|8530813_17:00:00| 600.0|\n",
      "|8591119_16:50:00|8503083_16:57:00| 420.0|\n",
      "|8591119_16:50:00|8591364_17:00:00| 600.0|\n",
      "|8591119_16:50:00|8591199_17:00:00| 600.0|\n",
      "|8591329_12:44:00|8503087_13:03:00|1140.0|\n",
      "|8591329_12:44:00|8591366_12:53:00| 540.0|\n",
      "|8591329_12:44:00|8591365_12:50:00| 360.0|\n",
      "+----------------+----------------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_edges_correspondance_walking_exploded = df_edges_correspondance_walking_exploded.withColumn('src', df_edges_correspondance_walking_exploded.edges.getItem(0))\n",
    "df_edges_correspondance_walking_exploded = df_edges_correspondance_walking_exploded.withColumn('dst', df_edges_correspondance_walking_exploded.edges.getItem(1)) \n",
    "df_edges_correspondance_walking_exploded = df_edges_correspondance_walking_exploded.withColumn('weight', df_edges_correspondance_walking_exploded.edges.getItem(2))\n",
    "df_edges_walk =  df_edges_correspondance_walking_exploded.drop('edges') \n",
    "df_edges_walk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save the correspondance edges to avoid recomputing them \n",
    "#df_edges_walk.write.csv(\"/user/chahed/edges_walk_filtered_final_corrected_bis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the all the pieces of our graph and this step needs only to be done once. Every computation is saved and we can ftom now on load it and directly compute the routing alogrithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and apply bfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as functions\n",
    "import datetime\n",
    "import pandas as pd \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf\n",
    "def date_formatting(date):\n",
    "    return '2019-05-13'+' '+date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -cat /user/kooli/dict_edges_simple.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/kooli/dict_edges_corr.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/kooli/pd_vertices.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/kooli/df_index.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/kooli/dict_walking_neighbors.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/kooli/dict_walk_time.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/kooli/dict_stop_list_hours.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/kooli/pd_delay_dist1yearmeanperh.pkl\n",
      "Success"
     ]
    }
   ],
   "source": [
    "dict_edges_bus = read_hdfs('/user/{}/dict_edges_simple.pkl'.format('kooli'))\n",
    "dict_edges_waiting = read_hdfs('/user/{}/dict_edges_corr.pkl'.format('kooli'))\n",
    "vertices = read_hdfs('/user/{}/pd_vertices.pkl'.format('kooli')) \n",
    "df_index = read_hdfs('/user/{}/df_index.pkl'.format('kooli'))\n",
    "walking_neighbors = read_hdfs('/user/{}/dict_walking_neighbors.pkl'.format('kooli') )\n",
    "dict_walk_time = read_hdfs('/user/{}/dict_walk_time.pkl'.format('kooli'))\n",
    "dict_stop_list_hours =  read_hdfs('/user/{}/dict_stop_list_hours.pkl'.format('kooli'))\n",
    "df_means = read_hdfs('/user/{}/pd_delay_dist1yearmeanperh.pkl'.format('kooli'))\n",
    "\n",
    "\n",
    "#format the vertices data frame\n",
    "vertices['station'] =  vertices.id.map(lambda x :  x.split('_')[0])\n",
    "vertices['time'] = vertices.id.map(lambda x :  x.split('_')[1])\n",
    "vertices['time_formatted'] = vertices.time.map(lambda x : datetime.datetime.strptime('2019-05-13'+' '+x, \"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is based on Breadth First Search with a few changes. We made the assumption that a trip shouldn't last more than 2 hours and that in each correspondence we can't walk more than 10 minutes. Since the graph nodes are event based (station + time) we set the destination nodes as the 5 last events occuring at the destination station strictly before the arrival time. We also made the assumption that we don't want to arrive exactly at the arrival time since this would cause the risk of being late to be very high. In order to provide a faster calculation (and unfortunately a suboptimal solution) we set as parameters a maximal depth for the tree traversal, we also set as parameter a minimum number of paths, once we surpass that threshold, the time window is updated from 2 hours to (**arrival time - earliest departure of the collected paths**). Another pruning method was to have a dictionary of **visited stations** and their time of visit. If we already visited a station at a cetain time **T** we only add a node from the same station if the time is superior to <em>**T**</em> since this means that there is a shorter path from that station to the destination. To ensure an output of different paths, we make sure that there are no two paths that have the same nodes except the source nodes, since this means that if path **A** starts at <em>**T**<sub>departure</sub></em> then path **B** is just waiting at the source station then taking path **A** at <em>**T**<sub>departure</sub></em>. So we only retain path **A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bfs_homies(source,final_stop_id, max_arrival_time, dict_edges_bus,dict_edges_waiting,vertices,df_index,walking_neighbors,\n",
    "               dict_walk_time,dict_stop_list_hours,max_iter=12,minimum_paths=5):\n",
    "    \n",
    "    #find the destination nodes\n",
    "    arrival_time_formatted = datetime.datetime.strptime('2019-05-13'+' '+max_arrival_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    possible_dest_nodes=vertices.loc[(vertices.time_formatted<arrival_time_formatted) & \\\n",
    "                                                      (vertices.station==final_stop_id),\"id\"]\n",
    "    \n",
    "    destinations=set(possible_dest_nodes.sort_values(ascending = False).values[:5]) \n",
    "    # first iteration will be to visit the destination nodes\n",
    "    to_visit=destinations.copy()\n",
    "    \n",
    "    # Heuristic rule: a trip duration cannot exceed 2 hours\n",
    "    max_time_formatted = datetime.datetime.strptime('2019-05-13'+' '+max_arrival_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    earliest_departure_time=max_time_formatted- datetime.timedelta(minutes = 120)\n",
    "    \n",
    "    # finding all the nodes correspending to the source station and possible departure times \n",
    "    possible_nodes_to_visit = vertices.loc[(vertices.time_formatted>earliest_departure_time) \\\n",
    "                                            & (vertices.time_formatted<max_time_formatted)]\n",
    "    \n",
    "    possible_sources = possible_nodes_to_visit.loc[possible_nodes_to_visit.station==source,\"id\"]\n",
    "                                           \n",
    "    possible_sources = set(possible_sources.values)\n",
    "    \n",
    "    \n",
    "    # Initialization\n",
    "    true_paths=[]\n",
    "    transports=[]\n",
    "    trunc_paths={}\n",
    "    sources_with_path=[]\n",
    "    dict_paths={}\n",
    "    visited_stations={}\n",
    "    \n",
    "    # add destination node\n",
    "    visited_stations[final_stop_id]=max_arrival_time\n",
    "    \n",
    "    break_condition=False\n",
    "    k=0   #iteration counter\n",
    "    while((not break_condition)&(k<max_iter)):\n",
    "        k+=1\n",
    "\n",
    "        # deep copy of the nodes to visit\n",
    "        to_visit_tmp=to_visit.copy()\n",
    "        to_visit=set()\n",
    "       \n",
    "        for node in to_visit_tmp:\n",
    "            \n",
    "            to_visit_next_from_node=set()\n",
    "            \n",
    "            stop_id, time = node.split('_')\n",
    "            visited_stations[stop_id]=time\n",
    "            node_transport=\"waiting\"\n",
    "            if (stop_id !=final_stop_id):\n",
    "                node_transport=dict_paths[node][1]\n",
    "            \n",
    "            \n",
    "            ################################ WALKING ######################\n",
    "            \n",
    "            #avoid double walking\n",
    "            if (node_transport!=\"walking\"):\n",
    "\n",
    "                time_formatted = datetime.datetime.strptime('2019-05-13'+' '+time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                index_node = df_index.loc[stop_id,'index'] \n",
    "                walking_nodes=walking_neighbors[stop_id]\n",
    "                for station_id_other in walking_nodes:  #iterate over the other nodes \n",
    "\n",
    "                    walking_time = datetime.timedelta(minutes = dict_walk_time[(stop_id, station_id_other)] )\n",
    "                    departure_time = time_formatted - walking_time\n",
    "\n",
    "                    hours_other = dict_stop_list_hours[station_id_other]\n",
    "                    possible_changes = list(filter( (lambda x : (x < departure_time) &(earliest_departure_time<x)) , \\\n",
    "                                            map(lambda x: datetime.datetime.strptime('2019-05-13'+' '+x,\"%Y-%m-%d %H:%M:%S\"),\\\n",
    "                                                hours_other) ))\n",
    "\n",
    "                    if(len(possible_changes) != 0):\n",
    "                                                \n",
    "                        first_hour_change = max(possible_changes)               \n",
    "                        #destination node\n",
    "                        first_hour_change_str = first_hour_change.strftime('%H:%M:%S')\n",
    "                        node_id_other = station_id_other+'_'+first_hour_change_str\n",
    "\n",
    "                        time_transfer = (first_hour_change - time_formatted).total_seconds()\n",
    "\n",
    "                        # add the node\n",
    "                        if (not (node_id_other in to_visit_tmp)):\n",
    "                            # check if the we reached the station before\n",
    "                            if station_id_other in visited_stations.keys():\n",
    "                                # if we reached it at a better time update it\n",
    "                                if first_hour_change_str>visited_stations[station_id_other]:\n",
    "                                    visited_stations[station_id_other]=first_hour_change_str\n",
    "                                    to_visit_next_from_node.add(node_id_other)\n",
    "                            else:\n",
    "                                to_visit_next_from_node.add(node_id_other)\n",
    "\n",
    "                        # if the path is arleady added, we don't update since we favoritize bus transport    \n",
    "                        if not (node_id_other in dict_paths.keys()):\n",
    "\n",
    "                            dict_paths[node_id_other]=[node,\"walking\",stop_id] # node=stop_id+time\n",
    "\n",
    "            #################### Bus ################\n",
    "            if node in dict_edges_bus.keys():\n",
    "\n",
    "                bus_nodes=dict_edges_bus[node]\n",
    "\n",
    "                for bus_node in bus_nodes:\n",
    "                    bus_node_id,bus_node_time=bus_node.split('_')\n",
    "                    \n",
    "                    if bus_node_id in visited_stations.keys():\n",
    "                        # if we reached it at a better time update it\n",
    "                        if bus_node_time>visited_stations[bus_node_id]:\n",
    "                            visited_stations[bus_node_id]=bus_node_time\n",
    "                            to_visit_next_from_node.add(bus_node)\n",
    "                            dict_paths[bus_node]=[node,\"bus\",stop_id]\n",
    "                    else:\n",
    "                        to_visit_next_from_node.add(bus_node)\n",
    "                        dict_paths[bus_node]=[node,\"bus\",stop_id]        \n",
    "                    # here we always update since this can only be better\n",
    "                    \n",
    "\n",
    "            #################### Correspendance ##############\n",
    "            if node in dict_edges_waiting.keys():\n",
    "                waiting_nodes=dict_edges_waiting[node]\n",
    "                to_visit_next_from_node.update(waiting_nodes)\n",
    "                \n",
    "                # here we always update since this can only be better\n",
    "                for waiting_node in waiting_nodes:\n",
    "                    waiting_node_id,waiting_node_time=waiting_node.split('_')\n",
    "                    \n",
    "                    if waiting_node_id in visited_stations.keys():\n",
    "                        # if we reached it at a better time update it\n",
    "                        if waiting_node_time>visited_stations[waiting_node_id]:\n",
    "                            visited_stations[waiting_node_id]=waiting_node_time\n",
    "                            \n",
    "                    dict_paths[waiting_node]=[node,\"waiting\",stop_id]\n",
    "            \n",
    "            # check if new nodes are valid\n",
    "            valid_nodes = filter((lambda x :(earliest_departure_time<datetime.datetime.strptime('2019-05-13'+' '+x.split('_')[1],\n",
    "                                                                                               \"%Y-%m-%d %H:%M:%S\"))) , \\\n",
    "                                            to_visit_next_from_node)\n",
    "                      \n",
    "            to_visit_next_from_node=set()                \n",
    "            for new_node in valid_nodes:\n",
    "                \n",
    "                if (new_node in possible_sources):\n",
    "                    \n",
    "                    # reconstruct the path\n",
    "                    double_walk=False # for not having two consecutive walking edges\n",
    "                    new_node_trans=dict_paths[new_node][1]\n",
    "                    last_transport=new_node_trans\n",
    "                    correct_path=[new_node]\n",
    "                    trans=[last_transport]\n",
    "                    trunc_path=[]\n",
    "                    trunc_trans=[]\n",
    "                    backprop=node\n",
    "                    current_station=dict_paths[new_node][2]\n",
    "                    path_stations=set()\n",
    "                    path_stations.add(current_station)\n",
    "                    loop=False # for not going to the same station twice\n",
    "                    nb=0 #number of hops, it shouldn't exceed 200\n",
    "                    while ((current_station!=final_stop_id)&(nb<200)&(not double_walk)&(not loop)):\n",
    "                        nb+=1\n",
    "                        node_meta=dict_paths[backprop]\n",
    "                        current_station=node_meta[2]\n",
    "                        \n",
    "                        if (current_station in path_stations):\n",
    "                            loop=True\n",
    "                        else:\n",
    "                            if ((node_meta[1]==last_transport==\"walking\")):\n",
    "                                double_walk=True\n",
    "                            else:\n",
    "                                path_stations.add(current_station)\n",
    "                                correct_path.append(backprop)\n",
    "                                trunc_path.append(backprop)\n",
    "                                \n",
    "                                backprop=node_meta[0]\n",
    "                                \n",
    "                                last_transport=node_meta[1]\n",
    "                                trans.append(last_transport)\n",
    "                                trunc_trans.append(last_transport)\n",
    "                                \n",
    "                    if(nb==200):\n",
    "                        print(\"Maximum hops reached at: \",correct_path)\n",
    "                    if((not double_walk)&(not loop)):\n",
    "                        trunc_key=tuple(trunc_path)\n",
    "                        if (trunc_key in trunc_paths.keys()):\n",
    "                            other_source=trunc_paths[trunc_key]\n",
    "                            new_time=new_node.split(\"_\")[1]\n",
    "                            other_time=other_source.split(\"_\")[1]\n",
    "                            if new_time>other_time:\n",
    "                                \n",
    "                                # remove the old path\n",
    "                                path_to_remove=trunc_path[:]\n",
    "                                path_to_remove.insert(0,other_source)\n",
    "                                path_to_remove.append(backprop)\n",
    "                                \n",
    "                                # remove the old transport\n",
    "                                other_transport=dict_paths[other_source][1]\n",
    "                                trans_to_remove=trunc_trans[:]\n",
    "                                trans_to_remove.insert(0,other_transport)\n",
    "                                trans_to_remove.append(last_transport)\n",
    "                                if path_to_remove in true_paths:\n",
    "                                    print(\"Old path starts at {}, better start at {} \".format(other_time,new_time))\n",
    "                                    true_paths.remove(path_to_remove)\n",
    "                                    transports.remove(trans_to_remove)\n",
    "                                else:\n",
    "                                    print(\"can't find path:\", path_to_remove)\n",
    "                                # update dict\n",
    "                                trunc_paths[trunc_key]=new_node\n",
    "                                # add updated path\n",
    "                                correct_path.append(backprop)\n",
    "                                trans.append(last_transport)\n",
    "                                true_paths.append(correct_path)\n",
    "                                transports.append(trans)\n",
    "                                sources_with_path.remove(other_source)\n",
    "                                sources_with_path.append(new_node)\n",
    "                                \n",
    "                        else:        \n",
    "                            trunc_paths[trunc_key]=new_node\n",
    "                            correct_path.append(backprop)\n",
    "                            trans.append(last_transport)\n",
    "                            true_paths.append(correct_path)\n",
    "                            transports.append(trans)\n",
    "                            sources_with_path.append(new_node)\n",
    "                            print(\"found a new path at iteration \",k)\n",
    "                            \n",
    "                else:\n",
    "                    to_visit_next_from_node.add(new_node)\n",
    "                  \n",
    "  \n",
    "            # don't add nodes that we are currently exploring\n",
    "            to_visit_next_from_node=to_visit_next_from_node-to_visit_tmp\n",
    "            \n",
    "            # cumulating nodes\n",
    "            to_visit.update(to_visit_next_from_node)\n",
    "        if (len(to_visit)>0):\n",
    "            selected_nodes=pd.DataFrame(list(map(lambda x: x.split(\"_\"),(to_visit)))). \\\n",
    "                                groupby(0).max().reset_index().apply(lambda x: x[0]+\"_\"+x[1],axis=1).values\n",
    "            \n",
    "            print(\"at iteration {} we are visiting {} nodes that corresponds to {} unique stations\".\n",
    "                  format(k,len(to_visit),len(selected_nodes)))\n",
    "        else:\n",
    "            # break if the all the graph is visited\n",
    "            break_condition=True\n",
    "            \n",
    "        #heuristic rule : limit the minimum number of paths\n",
    "        if (len(sources_with_path) >= min(minimum_paths,len(possible_sources))) :\n",
    "            earliest_time=sorted(sources_with_path)[0].split(\"_\")[1]\n",
    "            earliest_formatted = datetime.datetime.strptime('2019-05-13'+' '+earliest_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "            if earliest_formatted>earliest_departure_time:\n",
    "                print(\"update time\")\n",
    "                earliest_departure_time=earliest_formatted\n",
    "              \n",
    "    print(\"completed after {} iterations. Constructed tree with {} nodes. Found {} paths\".\n",
    "          format(k,len(dict_paths),len(true_paths)))\n",
    "    \n",
    "    if(len(sources_with_path) == 0):\n",
    "        print(\"No path found, try to increase BFS's depth.\")\n",
    "    else: \n",
    "        earliest_time = sorted(sources_with_path)[0].split(\"_\")[1]\n",
    "        earliest_formatted = datetime.datetime.strptime('2019-05-13'+' '+earliest_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if ((earliest_formatted==earliest_departure_time)|(len(to_visit)>0)):\n",
    "            print(\"If you can be more patient we can get you more paths by increasing the minimum path parameter\")\n",
    "        else:\n",
    "            print(\"That's all what we can find\")\n",
    "    return true_paths ,transports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take into account the risk factor, we first computed the hourly mean delay for all the bus stops. Assuming that this corresponds to the expected delay, we can modelize the delay distribution as an exponential with parameter $\\lambda = \\frac{1}{mean}$ . Then for each path we see when a correspondance happens <em>(i.e when the traveler gets off the bus to take the next one)</em> and get the mean of the delay for the last stop, we then see how much time does the path allow as delay to still catch the next bus, which is modelised as **T<sub>i</sub>=transfer time**. Now we can get the probability of not missing a correspondance at a correspondance $i$ as $P_i = 1 - e^{- \\lambda_{i} * T_i}$ . The final probability is therefore the product of $P_i's$ , we therefore compare it to the threshold to determine if the path is safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_safe_paths(true_paths,transports,max_time_arrival,threshold,df_means):\n",
    "    max_time_arrival_formatted = datetime.datetime.strptime('2019-05-13'+' '+max_time_arrival, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    path_safe = []\n",
    "    safe_probas=[]\n",
    "    for path,transport in zip(true_paths, transports) : \n",
    "\n",
    "        proba_success_transfer = 1\n",
    "\n",
    "        if(len(path) == 1):\n",
    "            pritn('Error need at least 2 stations !')\n",
    "        elif(len(path) == 2):\n",
    "\n",
    "            now = path[0]\n",
    "            after = path[1]\n",
    "            transport_now_after = transports[0]\n",
    "\n",
    "            if(transport == 'bus'):\n",
    "                station_now, time_now = now.split('_')\n",
    "                station_after, time_after = after.split('_')\n",
    "\n",
    "                hour_now = int(time_now[0:2])\n",
    "                hour_after = int(time_after[0:2])\n",
    "\n",
    "                mean = df_meas.loc[(df_means['stop_id_prev'] == station_now) & (df_means['arrival_prev_hour'] == hour_now) \n",
    "                                          & (df_means['stop_id_next'] == station_after) & (df_means['arrival_next_hour'] == hour_after),'mean_val'].values\n",
    "\n",
    "                if(mean.isEmpty()):\n",
    "                    lambda_ = 1e12  #equivlaent to infinity\n",
    "                else : \n",
    "                    lambda_ = 1/max(mean[0], 1e-12)\n",
    "\n",
    "                proba_success_transfer = proba_success_transfer * (1 - math.exp(-transfer_time * lambda_))\n",
    "\n",
    "        else :\n",
    "\n",
    "            for i in range(len(path)):  \n",
    "\n",
    "                if (i <= len(path) - 3) : \n",
    "\n",
    "                    prev = path[i]\n",
    "                    now = path[i+1]\n",
    "                    after = path[i+2]\n",
    "                    transport_now_after = transport[i+1]\n",
    "                    destination = (i==(len(path)-3))\n",
    "\n",
    "                    station_prev, time_prev = prev.split('_')\n",
    "                    station_now, time_now = now.split('_')\n",
    "                    station_after, time_after = after.split('_')\n",
    "\n",
    "                    hour_prev = int(time_prev[0:2])\n",
    "                    hour_now = int(time_now[0:2])\n",
    "\n",
    "                    mean = df_means.loc[(df_means['stop_id_prev'] == station_prev) & (df_means['arrival_prev_hour'] == hour_prev) \n",
    "                                          & (df_means['stop_id_next'] == station_now) & (df_means['arrival_next_hour'] == hour_now),'mean_val'].values\n",
    "\n",
    "                    if(mean.size == 0):\n",
    "                        lambda_ = 1e12   #equivlaent to infinity\n",
    "                    else : \n",
    "                        lambda_ = 1/max(mean[0], 1e-12)\n",
    "\n",
    "\n",
    "                    time_prev_formatted = datetime.datetime.strptime('2019-05-13'+' '+time_prev, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    time_now_formatted = datetime.datetime.strptime('2019-05-13'+' '+time_now, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    time_after_formatted = datetime.datetime.strptime('2019-05-13'+' '+time_after, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                    if(transport_now_after == 'waiting'):\n",
    "                        transfer_time = (time_after_formatted - time_now_formatted)\n",
    "\n",
    "                        if not destination:\n",
    "                            transfer_time=transfer_time - datetime.timedelta(minutes = 2)\n",
    "\n",
    "                        transfer_time=transfer_time.total_seconds()\n",
    "                        proba_success_transfer = proba_success_transfer * (1 - math.exp(-transfer_time * lambda_))\n",
    "\n",
    "                    elif(transport_now_after == 'walking'):\n",
    "                        transfer_time = ( (time_after_formatted - time_now_formatted) \n",
    "                                         - datetime.timedelta(minutes = float(dict_walk_time[(station_now,station_after)])) )\n",
    "\n",
    "                        if destination:\n",
    "                            transfer_time = transfer_time + datetime.timedelta(minutes = 2)\n",
    "                        transfer_time = transfer_time.total_seconds()\n",
    "                        proba_success_transfer = proba_success_transfer * (1 - math.exp(-transfer_time * lambda_))\n",
    "\n",
    "                    elif (destination  &  (transport_now_after == 'bus')):\n",
    "\n",
    "                        transfer_time = max_time_arrival_formatted - time_after_formatted\n",
    "                        transfer_time = transfer_time.total_seconds()\n",
    "\n",
    "\n",
    "        if (threshold < proba_success_transfer): \n",
    "            safe_probas.append(proba_success_transfer)\n",
    "            path_safe.append([path,transport[:-1]])\n",
    "    return path_safe , safe_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_final_output(path_safe,safe_probas):\n",
    "    initial_node=[]\n",
    "    for path in path_safe:\n",
    "        initial_node.append(path[0][0])\n",
    "\n",
    "    ordered_nodes=sorted(initial_node)[::-1]\n",
    "    ordered_safe_paths=[]\n",
    "    for i in range(len(ordered_nodes)):\n",
    "        ordered_safe_paths.append(path_safe[initial_node.index(ordered_nodes[i])])\n",
    "\n",
    "\n",
    "    final_output=[]\n",
    "    for path ,transport in ordered_safe_paths:\n",
    "        final_path=[]\n",
    "        i=0\n",
    "        taking_bus=False\n",
    "        while (i<len(transport)):\n",
    "            if not(taking_bus):\n",
    "                final_path.append(path[i].split(\"_\"))\n",
    "                final_path.append(transport[i])\n",
    "                if transport[i]==\"bus\":\n",
    "                    taking_bus=True\n",
    "            else:\n",
    "                if transport[i]!=\"bus\":\n",
    "                    taking_bus=False\n",
    "                    final_path.append(path[i].split(\"_\"))\n",
    "                    final_path.append(transport[i])\n",
    "            i+=1\n",
    "\n",
    "        final_path.append(path[-1].split(\"_\"))\n",
    "        final_output.append(final_path)\n",
    "\n",
    "\n",
    "    output_messages=[]\n",
    "    for index ,path in enumerate(final_output):\n",
    "\n",
    "        message='- Path number {} ({:0.2f}% certainty):\\n  Be at the starting station before {}.\\n'.format(\n",
    "            index+1,100*safe_probas[index],path[0][1])\n",
    "        for i in range(1,len(path),2):\n",
    "            if path[i]==\"bus\":\n",
    "                message+=\"  Take the bus from {} at {} to arrive to {} at {}\\n\".format(\n",
    "                    path[i-1][0],path[i-1][1],path[i+1][0],path[i+1][1])\n",
    "            elif path[i]==\"walking\":\n",
    "                message+=\"  Walk from {} to catch the bus at {} at {}\\n\".format(\n",
    "                path[i-1][0],path[i+1][0],path[i+1][1])\n",
    "            elif path[i]==\"waiting\":\n",
    "                message+=\"Wait at station {} to take the bus at {}\\n\".format(\n",
    "                path[i-1][0],path[i+1][1])\n",
    "        message+=\"You will arrive at your destination at {}\".format(path[len(path)-1][1]) \n",
    "        output_messages.append(message)\n",
    "    return final_output , output_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the inputs \n",
    "source = '8503000' \n",
    "destination_id = '8591049'\n",
    "max_time_arrival = '12:30:00'\n",
    "threshold = 0.9\n",
    "max_iter = 15\n",
    "minimum_paths = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run BFS to find the possible paths from the 2 stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 1 we are visiting 8 nodes that corresponds to 4 unique stations\n",
      "at iteration 2 we are visiting 12 nodes that corresponds to 6 unique stations\n",
      "at iteration 3 we are visiting 26 nodes that corresponds to 11 unique stations\n",
      "at iteration 4 we are visiting 38 nodes that corresponds to 19 unique stations\n",
      "at iteration 5 we are visiting 49 nodes that corresponds to 22 unique stations\n",
      "at iteration 6 we are visiting 102 nodes that corresponds to 29 unique stations\n",
      "('found a new path at iteration ', 7)\n",
      "Old path starts at 11:45:00, better start at 11:49:00 \n",
      "at iteration 7 we are visiting 160 nodes that corresponds to 43 unique stations\n",
      "('found a new path at iteration ', 8)\n",
      "('found a new path at iteration ', 8)\n",
      "('found a new path at iteration ', 8)\n",
      "('found a new path at iteration ', 8)\n",
      "at iteration 8 we are visiting 243 nodes that corresponds to 70 unique stations\n",
      "('found a new path at iteration ', 9)\n",
      "at iteration 9 we are visiting 344 nodes that corresponds to 96 unique stations\n",
      "('found a new path at iteration ', 10)\n",
      "Old path starts at 11:39:00, better start at 11:44:00 \n",
      "at iteration 10 we are visiting 441 nodes that corresponds to 147 unique stations\n",
      "('found a new path at iteration ', 11)\n",
      "('found a new path at iteration ', 11)\n",
      "at iteration 11 we are visiting 615 nodes that corresponds to 196 unique stations\n",
      "('found a new path at iteration ', 12)\n",
      "('found a new path at iteration ', 12)\n",
      "Old path starts at 11:09:00, better start at 11:14:00 \n",
      "at iteration 12 we are visiting 711 nodes that corresponds to 256 unique stations\n",
      "('found a new path at iteration ', 13)\n",
      "at iteration 13 we are visiting 887 nodes that corresponds to 322 unique stations\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "('found a new path at iteration ', 14)\n",
      "at iteration 14 we are visiting 1076 nodes that corresponds to 344 unique stations\n",
      "update time\n",
      "('found a new path at iteration ', 15)\n",
      "('found a new path at iteration ', 15)\n",
      "('found a new path at iteration ', 15)\n",
      "('found a new path at iteration ', 15)\n",
      "('found a new path at iteration ', 15)\n",
      "at iteration 15 we are visiting 979 nodes that corresponds to 356 unique stations\n",
      "completed after 15 iterations. Constructed tree with 8899 nodes. Found 26 paths\n",
      "If you can be more patient we can get you more paths by increasing the minimum path parameter"
     ]
    }
   ],
   "source": [
    "true_paths, transports = bfs_homies(source,destination_id, max_time_arrival, dict_edges_bus,\n",
    "                                    dict_edges_waiting,vertices,df_index,walking_neighbors,\n",
    "                                   dict_walk_time,dict_stop_list_hours,max_iter,minimum_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_safe , safe_probas = get_safe_paths(true_paths,transports,max_time_arrival,threshold,df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_output , output_messages = get_final_output(path_safe,safe_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Path number 1 (100.00% certainty):\n",
      "  Be at the starting station before 12:10:00.\n",
      "  Take the bus from 8503000 at 12:10:00 to arrive to 8503006 at 12:16:00\n",
      "  Walk from 8503006 to catch the bus at 8580449 at 12:21:00\n",
      "  Take the bus from 8580449 at 12:21:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 2 (99.99% certainty):\n",
      "  Be at the starting station before 12:09:00.\n",
      "  Take the bus from 8503000 at 12:09:00 to arrive to 8503006 at 12:15:00\n",
      "  Walk from 8503006 to catch the bus at 8591063 at 12:22:00\n",
      "  Take the bus from 8591063 at 12:22:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 3 (100.00% certainty):\n",
      "  Be at the starting station before 12:05:00.\n",
      "  Take the bus from 8503000 at 12:05:00 to arrive to 8503006 at 12:11:00\n",
      "  Walk from 8503006 to catch the bus at 8591382 at 12:18:00\n",
      "  Take the bus from 8591382 at 12:18:00 to arrive to 8591049 at 12:26:00\n",
      "You will arrive at your destination at 12:26:00\n",
      "\n",
      "- Path number 4 (100.00% certainty):\n",
      "  Be at the starting station before 12:02:00.\n",
      "  Take the bus from 8503000 at 12:02:00 to arrive to 8503310 at 12:17:00\n",
      "  Walk from 8503310 to catch the bus at 8590620 at 12:23:00\n",
      "  Take the bus from 8590620 at 12:23:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 5 (100.00% certainty):\n",
      "  Be at the starting station before 11:49:00.\n",
      "  Take the bus from 8503000 at 11:49:00 to arrive to 8503006 at 11:54:00\n",
      "  Walk from 8503006 to catch the bus at 8591063 at 12:02:00\n",
      "  Take the bus from 8591063 at 12:02:00 to arrive to 8591256 at 12:03:00\n",
      "  Walk from 8591256 to catch the bus at 8591294 at 12:13:00\n",
      "  Take the bus from 8591294 at 12:13:00 to arrive to 8591830 at 12:14:00\n",
      "  Walk from 8591830 to catch the bus at 8591128 at 12:25:00\n",
      "  Take the bus from 8591128 at 12:25:00 to arrive to 8591049 at 12:26:00\n",
      "You will arrive at your destination at 12:26:00\n",
      "\n",
      "- Path number 6 (100.00% certainty):\n",
      "  Be at the starting station before 11:47:00.\n",
      "  Take the bus from 8503000 at 11:47:00 to arrive to 8503340 at 12:00:00\n",
      "  Walk from 8503340 to catch the bus at 8590620 at 12:23:00\n",
      "  Take the bus from 8590620 at 12:23:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 7 (100.00% certainty):\n",
      "  Be at the starting station before 11:47:00.\n",
      "  Take the bus from 8503000 at 11:47:00 to arrive to 8503340 at 12:00:00\n",
      "  Walk from 8503340 to catch the bus at 8590620 at 12:23:00\n",
      "  Take the bus from 8590620 at 12:23:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 8 (91.27% certainty):\n",
      "  Be at the starting station before 11:46:00.\n",
      "  Walk from 8503000 to catch the bus at 8591368 at 11:54:00\n",
      "  Take the bus from 8591368 at 11:54:00 to arrive to 8594239 at 12:00:00\n",
      "  Walk from 8594239 to catch the bus at 8503020 at 12:09:00\n",
      "  Take the bus from 8503020 at 12:09:00 to arrive to 8503310 at 12:17:00\n",
      "  Walk from 8503310 to catch the bus at 8590620 at 12:23:00\n",
      "  Take the bus from 8590620 at 12:23:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 9 (93.65% certainty):\n",
      "  Be at the starting station before 11:45:00.\n",
      "  Take the bus from 8503000 at 11:45:00 to arrive to 8503147 at 11:55:00\n",
      "  Walk from 8503147 to catch the bus at 8591271 at 12:06:00\n",
      "  Take the bus from 8591271 at 12:06:00 to arrive to 8591349 at 12:10:00\n",
      "  Walk from 8591349 to catch the bus at 8591188 at 12:19:00\n",
      "  Take the bus from 8591188 at 12:19:00 to arrive to 8591297 at 12:21:00\n",
      "  Walk from 8591297 to catch the bus at 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 10 (100.00% certainty):\n",
      "  Be at the starting station before 11:45:00.\n",
      "  Take the bus from 8503000 at 11:45:00 to arrive to 8503147 at 11:55:00\n",
      "  Walk from 8503147 to catch the bus at 8591271 at 12:06:00\n",
      "  Take the bus from 8591271 at 12:06:00 to arrive to 8591349 at 12:10:00\n",
      "  Walk from 8591349 to catch the bus at 8591188 at 12:19:00\n",
      "  Take the bus from 8591188 at 12:19:00 to arrive to 8591297 at 12:21:00\n",
      "  Walk from 8591297 to catch the bus at 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 11 (100.00% certainty):\n",
      "  Be at the starting station before 11:44:00.\n",
      "  Take the bus from 8503000 at 11:44:00 to arrive to 8503006 at 11:51:00\n",
      "  Walk from 8503006 to catch the bus at 8580449 at 11:55:00\n",
      "  Take the bus from 8580449 at 11:55:00 to arrive to 8591175 at 11:59:00\n",
      "  Walk from 8591175 to catch the bus at 8591318 at 12:11:00\n",
      "  Take the bus from 8591318 at 12:11:00 to arrive to 8591225 at 12:12:00\n",
      "  Walk from 8591225 to catch the bus at 8591049 at 12:27:00\n",
      "You will arrive at your destination at 12:27:00\n",
      "\n",
      "- Path number 12 (96.38% certainty):\n",
      "  Be at the starting station before 11:35:00.\n",
      "  Take the bus from 8503000 at 11:35:00 to arrive to 8503011 at 11:39:00\n",
      "  Walk from 8503011 to catch the bus at 8591218 at 11:53:00\n",
      "  Take the bus from 8591218 at 11:53:00 to arrive to 8591177 at 11:59:00\n",
      "  Walk from 8591177 to catch the bus at 8503020 at 12:09:00\n",
      "  Take the bus from 8503020 at 12:09:00 to arrive to 8503310 at 12:17:00\n",
      "  Walk from 8503310 to catch the bus at 8590620 at 12:23:00\n",
      "  Take the bus from 8590620 at 12:23:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 13 (96.38% certainty):\n",
      "  Be at the starting station before 11:32:00.\n",
      "  Take the bus from 8503000 at 11:32:00 to arrive to 8503147 at 11:41:00\n",
      "  Walk from 8503147 to catch the bus at 8591271 at 11:57:00\n",
      "  Take the bus from 8591271 at 11:57:00 to arrive to 8591159 at 11:59:00\n",
      "  Walk from 8591159 to catch the bus at 8591325 at 12:09:00\n",
      "  Take the bus from 8591325 at 12:09:00 to arrive to 8591349 at 12:10:00\n",
      "  Walk from 8591349 to catch the bus at 8591188 at 12:19:00\n",
      "  Take the bus from 8591188 at 12:19:00 to arrive to 8591297 at 12:21:00\n",
      "  Walk from 8591297 to catch the bus at 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 14 (94.34% certainty):\n",
      "  Be at the starting station before 11:16:00.\n",
      "  Take the bus from 8503000 at 11:16:00 to arrive to 8503011 at 11:24:00\n",
      "  Walk from 8503011 to catch the bus at 8591227 at 11:42:00\n",
      "  Take the bus from 8591227 at 11:42:00 to arrive to 8591448 at 11:46:00\n",
      "  Walk from 8591448 to catch the bus at 8591038 at 11:56:00\n",
      "  Take the bus from 8591038 at 11:56:00 to arrive to 8591177 at 11:59:00\n",
      "  Walk from 8591177 to catch the bus at 8503020 at 12:09:00\n",
      "  Take the bus from 8503020 at 12:09:00 to arrive to 8503310 at 12:17:00\n",
      "  Walk from 8503310 to catch the bus at 8590620 at 12:23:00\n",
      "  Take the bus from 8590620 at 12:23:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 15 (100.00% certainty):\n",
      "  Be at the starting station before 11:16:00.\n",
      "  Take the bus from 8503000 at 11:16:00 to arrive to 8503011 at 11:24:00\n",
      "  Walk from 8503011 to catch the bus at 8591227 at 11:42:00\n",
      "  Take the bus from 8591227 at 11:42:00 to arrive to 8591448 at 11:46:00\n",
      "  Walk from 8591448 to catch the bus at 8591038 at 11:56:00\n",
      "  Take the bus from 8591038 at 11:56:00 to arrive to 8591177 at 11:59:00\n",
      "  Walk from 8591177 to catch the bus at 8503020 at 12:09:00\n",
      "  Take the bus from 8503020 at 12:09:00 to arrive to 8503310 at 12:17:00\n",
      "  Walk from 8503310 to catch the bus at 8590620 at 12:23:00\n",
      "  Take the bus from 8590620 at 12:23:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 16 (100.00% certainty):\n",
      "  Be at the starting station before 11:14:00.\n",
      "  Take the bus from 8503000 at 11:14:00 to arrive to 8503015 at 11:17:00\n",
      "  Walk from 8503015 to catch the bus at 8591323 at 11:40:00\n",
      "  Take the bus from 8591323 at 11:40:00 to arrive to 8594239 at 11:42:00\n",
      "  Walk from 8594239 to catch the bus at 8503020 at 11:51:00\n",
      "  Take the bus from 8503020 at 11:51:00 to arrive to 8503309 at 12:02:00\n",
      "  Walk from 8503309 to catch the bus at 8587799 at 12:19:00\n",
      "  Take the bus from 8587799 at 12:19:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 17 (96.38% certainty):\n",
      "  Be at the starting station before 11:14:00.\n",
      "  Take the bus from 8503000 at 11:14:00 to arrive to 8503015 at 11:17:00\n",
      "  Walk from 8503015 to catch the bus at 8591323 at 11:40:00\n",
      "  Take the bus from 8591323 at 11:40:00 to arrive to 8594239 at 11:42:00\n",
      "  Walk from 8594239 to catch the bus at 8503020 at 11:51:00\n",
      "  Take the bus from 8503020 at 11:51:00 to arrive to 8503309 at 12:02:00\n",
      "  Walk from 8503309 to catch the bus at 8587799 at 12:19:00\n",
      "  Take the bus from 8587799 at 12:19:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 18 (99.99% certainty):\n",
      "  Be at the starting station before 11:08:00.\n",
      "  Walk from 8503000 to catch the bus at 8591379 at 11:16:00\n",
      "  Take the bus from 8591379 at 11:16:00 to arrive to 8591168 at 11:22:00\n",
      "  Walk from 8591168 to catch the bus at 8591276 at 11:29:00\n",
      "  Take the bus from 8591276 at 11:29:00 to arrive to 8591393 at 11:31:00\n",
      "  Walk from 8591393 to catch the bus at 8591140 at 11:43:00\n",
      "  Take the bus from 8591140 at 11:43:00 to arrive to 8591112 at 11:44:00\n",
      "  Walk from 8591112 to catch the bus at 8591273 at 11:58:00\n",
      "  Take the bus from 8591273 at 11:58:00 to arrive to 8591175 at 11:59:00\n",
      "  Walk from 8591175 to catch the bus at 8591318 at 12:11:00\n",
      "  Take the bus from 8591318 at 12:11:00 to arrive to 8591225 at 12:12:00\n",
      "  Walk from 8591225 to catch the bus at 8591049 at 12:27:00\n",
      "You will arrive at your destination at 12:27:00\n",
      "\n",
      "- Path number 19 (99.98% certainty):\n",
      "  Be at the starting station before 11:05:00.\n",
      "  Take the bus from 8503000 at 11:05:00 to arrive to 8503011 at 11:09:00\n",
      "  Walk from 8503011 to catch the bus at 8591381 at 11:27:00\n",
      "  Take the bus from 8591381 at 11:27:00 to arrive to 8591079 at 11:30:00\n",
      "  Walk from 8591079 to catch the bus at 8591184 at 11:37:00\n",
      "  Take the bus from 8591184 at 11:37:00 to arrive to 8591177 at 11:41:00\n",
      "  Walk from 8591177 to catch the bus at 8503020 at 11:51:00\n",
      "  Take the bus from 8503020 at 11:51:00 to arrive to 8503309 at 12:02:00\n",
      "  Walk from 8503309 to catch the bus at 8587799 at 12:19:00\n",
      "  Take the bus from 8587799 at 12:19:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 20 (92.83% certainty):\n",
      "  Be at the starting station before 11:05:00.\n",
      "  Take the bus from 8503000 at 11:05:00 to arrive to 8503011 at 11:09:00\n",
      "  Walk from 8503011 to catch the bus at 8591381 at 11:27:00\n",
      "  Take the bus from 8591381 at 11:27:00 to arrive to 8591079 at 11:30:00\n",
      "  Walk from 8591079 to catch the bus at 8591184 at 11:37:00\n",
      "  Take the bus from 8591184 at 11:37:00 to arrive to 8591177 at 11:41:00\n",
      "  Walk from 8591177 to catch the bus at 8503020 at 11:51:00\n",
      "  Take the bus from 8503020 at 11:51:00 to arrive to 8503309 at 12:02:00\n",
      "  Walk from 8503309 to catch the bus at 8587799 at 12:19:00\n",
      "  Take the bus from 8587799 at 12:19:00 to arrive to 8591049 at 12:29:00\n",
      "You will arrive at your destination at 12:29:00\n",
      "\n",
      "- Path number 21 (96.38% certainty):\n",
      "  Be at the starting station before 11:00:00.\n",
      "  Walk from 8503000 to catch the bus at 8587349 at 11:06:00\n",
      "  Take the bus from 8587349 at 11:06:00 to arrive to 8591291 at 11:10:00\n",
      "  Walk from 8591291 to catch the bus at 8591335 at 11:21:00\n",
      "  Take the bus from 8591335 at 11:21:00 to arrive to 8591168 at 11:22:00\n",
      "  Walk from 8591168 to catch the bus at 8591276 at 11:29:00\n",
      "  Take the bus from 8591276 at 11:29:00 to arrive to 8591393 at 11:31:00\n",
      "  Walk from 8591393 to catch the bus at 8591140 at 11:43:00\n",
      "  Take the bus from 8591140 at 11:43:00 to arrive to 8591112 at 11:44:00\n",
      "  Walk from 8591112 to catch the bus at 8591273 at 11:58:00\n",
      "  Take the bus from 8591273 at 11:58:00 to arrive to 8591175 at 11:59:00\n",
      "  Walk from 8591175 to catch the bus at 8591318 at 12:11:00\n",
      "  Take the bus from 8591318 at 12:11:00 to arrive to 8591225 at 12:12:00\n",
      "  Walk from 8591225 to catch the bus at 8591049 at 12:27:00\n",
      "You will arrive at your destination at 12:27:00\n",
      "\n",
      "- Path number 22 (100.00% certainty):\n",
      "  Be at the starting station before 10:59:00.\n",
      "  Take the bus from 8503000 at 10:59:00 to arrive to 8591291 at 11:10:00\n",
      "  Walk from 8591291 to catch the bus at 8591335 at 11:21:00\n",
      "  Take the bus from 8591335 at 11:21:00 to arrive to 8591168 at 11:22:00\n",
      "  Walk from 8591168 to catch the bus at 8591276 at 11:29:00\n",
      "  Take the bus from 8591276 at 11:29:00 to arrive to 8591393 at 11:31:00\n",
      "  Walk from 8591393 to catch the bus at 8591140 at 11:43:00\n",
      "  Take the bus from 8591140 at 11:43:00 to arrive to 8591112 at 11:44:00\n",
      "  Walk from 8591112 to catch the bus at 8591273 at 11:58:00\n",
      "  Take the bus from 8591273 at 11:58:00 to arrive to 8591175 at 11:59:00\n",
      "  Walk from 8591175 to catch the bus at 8591318 at 12:11:00\n",
      "  Take the bus from 8591318 at 12:11:00 to arrive to 8591225 at 12:12:00\n",
      "  Walk from 8591225 to catch the bus at 8591049 at 12:27:00\n",
      "You will arrive at your destination at 12:27:00\n",
      "\n",
      "- Path number 23 (94.34% certainty):\n",
      "  Be at the starting station before 10:53:00.\n",
      "  Walk from 8503000 to catch the bus at 8591379 at 11:01:00\n",
      "  Take the bus from 8591379 at 11:01:00 to arrive to 8591071 at 11:03:00\n",
      "  Walk from 8591071 to catch the bus at 8591237 at 11:14:00\n",
      "  Take the bus from 8591237 at 11:14:00 to arrive to 8591335 at 11:15:00\n",
      "  Walk from 8591335 to catch the bus at 8591168 at 11:27:00\n",
      "  Take the bus from 8591168 at 11:27:00 to arrive to 8591073 at 11:31:00\n",
      "  Walk from 8591073 to catch the bus at 8591140 at 11:43:00\n",
      "  Take the bus from 8591140 at 11:43:00 to arrive to 8591112 at 11:44:00\n",
      "  Walk from 8591112 to catch the bus at 8591273 at 11:58:00\n",
      "  Take the bus from 8591273 at 11:58:00 to arrive to 8591175 at 11:59:00\n",
      "  Walk from 8591175 to catch the bus at 8591318 at 12:11:00\n",
      "  Take the bus from 8591318 at 12:11:00 to arrive to 8591225 at 12:12:00\n",
      "  Walk from 8591225 to catch the bus at 8591049 at 12:27:00\n",
      "You will arrive at your destination at 12:27:00\n",
      "\n",
      "- Path number 24 (100.00% certainty):\n",
      "  Be at the starting station before 10:48:00.\n",
      "  Walk from 8503000 to catch the bus at 8591368 at 10:56:00\n",
      "  Take the bus from 8591368 at 10:56:00 to arrive to 8591282 at 10:57:00\n",
      "  Walk from 8591282 to catch the bus at 8591257 at 11:08:00\n",
      "  Take the bus from 8591257 at 11:08:00 to arrive to 8591291 at 11:10:00\n",
      "  Walk from 8591291 to catch the bus at 8591335 at 11:21:00\n",
      "  Take the bus from 8591335 at 11:21:00 to arrive to 8591168 at 11:22:00\n",
      "  Walk from 8591168 to catch the bus at 8591276 at 11:29:00\n",
      "  Take the bus from 8591276 at 11:29:00 to arrive to 8591393 at 11:31:00\n",
      "  Walk from 8591393 to catch the bus at 8591140 at 11:43:00\n",
      "  Take the bus from 8591140 at 11:43:00 to arrive to 8591112 at 11:44:00\n",
      "  Walk from 8591112 to catch the bus at 8591273 at 11:58:00\n",
      "  Take the bus from 8591273 at 11:58:00 to arrive to 8591175 at 11:59:00\n",
      "  Walk from 8591175 to catch the bus at 8591318 at 12:11:00\n",
      "  Take the bus from 8591318 at 12:11:00 to arrive to 8591225 at 12:12:00\n",
      "  Walk from 8591225 to catch the bus at 8591049 at 12:27:00\n",
      "You will arrive at your destination at 12:27:00"
     ]
    }
   ],
   "source": [
    "for msg in output_messages:\n",
    "    print(msg)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
